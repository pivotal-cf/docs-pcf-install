---
title: Advanced Troubleshooting with the BOSH CLI
owner: Ops Manager
---

<strong><%= modified_date %></strong>

To perform advanced troubleshooting, you must log in to the BOSH Director. From there, you can run specific commands using the BOSH Command Line Interface (CLI). BOSH Director diagnostic commands have access to information about your entire [Pivotal Cloud Foundry](https://network.pivotal.io/products/pivotal-cf) (PCF) installation.

The BOSH Director runs on the virtual machine (VM) that Ops Manager deploys on the first install of the Ops Manager Director tile.

BOSH Director diagnostic commands have access to information about your entire
[Pivotal Cloud Foundry](https://network.pivotal.io/products/pivotal-cf)
(PCF) installation.

<p class="note"><strong>Note</strong>: For more troubleshooting information, refer to the <a href="./troubleshooting.html">Troubleshooting Guide</a>.</p>

<p class="note"><strong>Note</strong>: Verify that no BOSH Director tasks are running on the Ops Manager VM before running any commands. You should
not proceed with troubleshooting until all BOSH Director tasks have completed or you
have ended them. See the <a href="https://bosh.io/docs/sysadmin-commands.html">BOSH CLI Commands</a> for more information.</p>

## <a id='prepare'></a>Prepare to Use the BOSH CLI

This section guides you through preparing to use the BOSH CLI.

### <a id='gather'></a> Gather Information

Before you begin troubleshooting with the BOSH CLI, collect the information you
need from the Ops Manager interface.

1. Open the Ops Manager interface by navigating to the Ops Manager fully qualified domain name (FQDN). Ensure that there are no installations or
updates in progress.

1. Click the **Ops Manager Director** tile and select the **Status** tab.

1. Record the IP address for the Director job. This is the IP
address of the VM where the BOSH Director runs.

	<%= image_tag("ops-mgr-job-ip.png") %>

1. Select the **Credentials** tab.

1. Click **Link to Credential** to view and record the **Director Credentials**.

    <%= image_tag("bosh-creds.png") %>

1. Return to the **Installation Dashboard**.

1. **(Optional)** To prepare to troubleshoot the job VM for any other product,
click the product tile and repeat the procedure above to record the IP address
and VM credentials for that job VM.

1. Log out of Ops Manager.

<p class="note"><strong>Note</strong>: You must log out of the Ops Manager interface to use the BOSH CLI.</p>


## <a id='ssh'></a> SSH Into Ops Manager VM

Use SSH to connect to the Ops Manager VM. Follow the instructions in one of the sections below to SSH into the VM.

### <a id='ssh-aws'></a> AWS

To SSH into the Ops Manager VM in AWS, you need the key pair you used when you created the Ops Manager VM.
To see the name of the key pair, click on the Ops Manager VM and locate the `key pair name` in the properties.

Perform the following steps to SSH into the Ops Manager VM in AWS:

1. Locate the Ops Manager FQDN on the AWS **EC2 instances** page.

1. Change the permissions on the `.pem` file to be more restrictive:

	<pre class="terminal">
	$ chmod 600 ops_mgr.pem
	</pre>

1. Run the `ssh` command:

	<pre class="terminal">
	ssh -i ops_mgr.pem ubuntu@OPS-MANAGER-FQDN
	</pre>

### <a id='ssh-azure'></a> Azure

To SSH into the Ops Manager VM in Azure, you need the key pair you used when creating the Ops Manager VM.
If you need to reset the SSH key, locate the Ops Manager VM in the Azure portal and click **Reset Password**.

Perform the following steps to SSH into the Ops Manager VM in Azure:

1. Locate the Ops Manager FQDN by selecting the VM in the Azure portal.

1. Run `chmod 600 ops_mgr.pem` to change the permissions on the `.pem` file to be more restrictive. For example:
  <pre class="terminal">
  $ chmod 600 ops_mgr.pem
  </pre>

1. Run `ssh -i ops_mgr.pem ubuntu@OPS-MANAGER-FQDN` to SSH into the VM. Replace `OPS-MANAGER-FQDN` with the fully qualified domain name of Ops Manager. For example:
  <pre class="terminal">
  $ ssh -i ops_mgr.pem ubuntu<span>@</span>my-opsmanager-fqdn.example.com
  </pre>

### <a id='ssh-gcp'></a> GCP

To SSH into the Ops Manager VM in GCP, follow these instructions:

1. Confirm that you have installed the [Google Cloud SDK and CLI](https://cloud.google.com/sdk/docs/quickstart-macos). See the [Google Cloud Platform documentation](https://cloud.google.com/sdk/gcloud/#downloading_gcloud) for more information.

1. Initialize Google Cloud CLI, using a user account with Owner, Editor, or Viewer permissions to access the project. Ensure that the Google Cloud CLI can login to the project by running the command `gcloud auth login`.

1. From the GCP web console, navigate to **Compute Engine**.

1. Locate the Ops Manager VM in the **VM Instances** list.

1. Under **Remote access**, click the **SSH** dropdown and select **View gcloud command**.

1. Copy the SSH command that appears in the popup window.

1. Paste the command into your terminal window to SSH to the VM. For example:
    <pre class="terminal">
    $ gcloud compute --project "YOUR-PROJECT-ID" ssh --zone "YOUR-ZONE-ID" "YOUR-VM"
    </pre>

### <a id='ssh-openstack'></a> OpenStack

To SSH into the Ops Manager VM in OpenStack, you need the key pair that you created in the [Configure Security](./openstack-setup.html#security) step of the _Provisioning the OpenStack Infrastructure_ topic.
If you need to reset the SSH key, locate the Ops Manager VM in the OpenStack console and boot it in recovery mode to generate a new key pair.

Perform the following steps to SSH into the Ops Manager VM in OpenStack:

1. Locate the Ops Manager FQDN on the **Access & Security** page.

1. Run `chmod 600 ops_mgr.pem` to change the permissions on the `.pem` file to be more restrictive. For example:
  <pre class="terminal">
  $ chmod 600 ops_mgr.pem
  </pre>

1. Run `ssh -i ops_mgr.pem ubuntu@OPS-MANAGER-FQDN` to SSH into the VM. Replace `OPS-MANAGER-FQDN` with the fully qualified domain name of Ops Manager. For example:
  <pre class="terminal">
  $ ssh -i ops_mgr.pem ubuntu<span>@</span>my-opsmanager-fqdn.example.com
  </pre>

### <a id='ssh-vsphere'></a> vSphere

To SSH into the Ops Manager VM in vSphere, you need the credentials used to import the PCF .ova or .ovf file into your virtualization system.
You set these credentials when you installed Ops Manager.


## <a id='log-in'></a> Log In to the BOSH Director

Log in to the BOSH Director using one of the following options:

- [Internal User Store Login via UAA](#log-in-uaa) - target and log in to the Director using BOSH.

- [External User Store Login via SAML](#log-in-saml) - use an external user store to log in to the BOSH Director.

#### <a id='log-in-uaa'></a> Internal User Store Login via UAA

1. Target the BOSH UAA on Ops Manager with the UAAC command `uaac target`.

    <pre class='terminal'>
    $ uaac target --ca-cert /var/tempest/workspaces/default/root_ca_certificate https://DIRECTOR-IP-ADDRESS:8443
    </pre>

1. Run `bosh target DIRECTOR-IP-ADDRESS` to target your Ops Manager VM using the BOSH CLI.

1. Retrieve the Director password from the **Ops Manager Director > Credentials** tab. Alternatively, launch a browser and visit the following URL to obtain the password:

    ```
    https://{OPSMANAGER-FQDN}/api/v0/deployed/director/credentials/director_credentials
    ```

1. Log in using the BOSH Director credentials:

    <pre class='terminal'>
    $ bosh --ca-cert /var/tempest/workspaces/default/root_ca_certificate target DIRECTOR-IP-ADDRESS
    Target set to 'DIRECTOR_UUID'
    Your username: director
    Enter password: (DIRECTOR_CREDENTIAL)
    Logged in as 'director'
    </pre>

#### <a id='log-in-saml'></a> External User Store Login via SAML

1. Log in to your identity provider and use the following information to configure SAML Service Provider Properties:
  * **Service Provider Entity ID**: `bosh-uaa`
  * **ACS URL**: `https://DIRECTOR-IP-ADDRESS:8443/saml/SSO/alias/bosh-uaa`
  * **Binding**: HTTP Post
  * **SLO URL**: `https://DIRECTOR-IP-ADDRESS:8443/saml/SSO/alias/bosh-uaa`
  * **Binding**: HTTP Redirect
  * **Name ID**: Email Address

1. Log in to BOSH using your SAML credentials:
    <pre class='terminal'>
    $ bosh login
    Email: admin
    Password:
    One Time Code (Get one at https:<span>//</span>192.0.2.16.11:8888/passcode):
    </pre>
    <p class="note"><strong>Note: </strong>Your browser must be able to reach the BOSH Director in order to log in with SAML.</p>

1. Click **Log in with organization credentials (SAML)**.

    <%= image_tag("login-saml-credentials.png") %>

1. Copy the **Temporary Authentication Code** that appears in your browser.

    <%= image_tag("saml-login-temp-auth-code.png") %>

1. You see a login confirmation. For example:

    <pre class='terminal'>Logged in as admin@example.org
    </pre>

### <a id='product'></a> Select a Product Deployment to Troubleshoot

When you import and install a product using Ops Manager, you deploy an instance
of the product described by a YAML file. Examples of available products include
Elastic Runtime, MySQL, or any other service that you imported and installed.

Perform the following steps to select a product deployment to troubleshoot:

1. Identify the YAML file that describes the deployment you want to
troubleshoot.

You identify the YAML file that describes a deployment by its filename.
For example, to identify Elastic Runtime deployments, run the following
command:

`find /var/tempest/workspaces/default/deployments -name cf-*.yml`

The table below shows the naming conventions for deployment files.

<table border="1" class="nice" >
<tr>
<th>Product</th><th>Deployment Filename Convention</th>
</tr>
<tr>
<td>Elastic Runtime</td>
<td>cf-&lt;20-character_random_string&gt;.yml</td>
</tr>
<tr>
<td>MySQL Dev</td>
<td>cf_services-&lt;20-character_random_string&gt;.yml</td>
</tr>
<tr>
<td>Other</td>
<td>&lt;20-character_random_string&gt;.yml</td>
</tr>
</table>

<p class="note"><strong>Note</strong>: Where there is more than one installation of the same product, record the release number shown on the product tile in Operations Manager. Then, from the YAML files for that product, find the deployment that specifies the same release version as the product tile.</p>

1. Run `bosh status` and record the UUID value.

1. Open the `DEPLOYMENT-FILENAME.yml` file in a text editor and compare the
`director_uuids` value in this file with the UUID value that you recorded. If
the values do not match, perform the following steps:

1. Replace the `director_uuids` value with the UUID value.

1. Run `bosh deployment DEPLOYMENT-FILENAME.yml` to reset the file for your
deployment.

1. Run `bosh deployment DEPLOYMENT-FILENAME.yml` to instruct the BOSH Director
to apply BOSH CLI commands against the deployment described by the YAML file that you identified:

<pre class='terminal'>
$ bosh deployment /var/tempest/workspaces/default/deployments/cf-cca1234abcd.yml
</pre>


## <a id='cli'></a> Use the BOSH CLI for Troubleshooting

This section describes three BOSH CLI commands commonly used during
troubleshooting.

* **VMS**: Lists all VMs in a deployment
* **Cloudcheck**: Runs a cloud consistency check and interactive repair
* **SSH**: Starts an interactive session or executes commands with a VM

### <a id='vms'></a> BOSH VMs

The `bosh2 vms` command provides an overview of the virtual machines that BOSH manages.

`bosh vms` provides an overview of the virtual machines that BOSH manages as part of the current deployment.

<pre class='terminal'>
$ bosh vms
Acting as user 'director' on 'p-bosh-e11111e1e023e2ee1e11'
RSA 1024 bit CA certificates are loaded due to old openssl compatibility
Deployment 'cf-33e333333eebbb3b33b3'
Director task 2002
Task 2002 done
+-------------------------------------------------------------------------------------------------------+---------+-----+--------------------------------------------------------------+------------+
| VM                                                                                                    | State   | AZ  | VM Type                                                      | IPs        |
+-------------------------------------------------------------------------------------------------------+---------+-----+--------------------------------------------------------------+------------+
| clock_global-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc111111111)                  | running | n/a | clock_global-partition-9965d7cc1758828b974f                  | 10.0.16.20 |
| cloud_controller-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc222222222)              | running | n/a | cloud_controller-partition-3333e3ee3332221e222e              | 10.0.16.19 |
| cloud_controller_worker-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc333333333)       | running | n/a | cloud_controller_worker-partition-3333e3ee3332221e222e       | 10.0.16.21 |
| consul_server-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc444444444)                 | running | n/a | consul_server-partition-3333e3ee3332221e222e                 | 10.0.16.11 |
| diego_brain-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc555555555)                   | running | n/a | diego_brain-partition-3333e3ee3332221e222e                   | 10.0.16.23 |
| diego_cell-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc666666666)                    | running | n/a | diego_cell-partition-3333e3ee3332221e222e                    | 10.0.16.24 |
| diego_cell-partition-3333e3ee3332221e222e/1 (abc31111-111e-1ec3-bb3e-ccc777777777)                    | running | n/a | diego_cell-partition-3333e3ee3332221e222e                    | 10.0.16.25 |
| diego_cell-partition-3333e3ee3332221e222e/2 (abc31111-111e-1ec3-bb3e-ccc888888888)                    | running | n/a | diego_cell-partition-3333e3ee3332221e222e                    | 10.0.16.26 |
| diego_database-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc999999999)                | running | n/a | diego_database-partition-3333e3ee3332221e222e                | 10.0.16.14 |
| doppler-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ddd111111111)                       | running | n/a | doppler-partition-3333e3ee3332221e222e                       | 10.0.16.27 |
| etcd_server-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-eee111111111)                   | running | n/a | etcd_server-partition-3333e3ee3332221e222e                   | 10.0.16.13 |
| loggregator_trafficcontroller-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-fff223111111) | running | n/a | loggregator_trafficcontroller-partition-3333e3ee3332221e222e | 10.0.16.28 |
| mysql-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ggg111111111)                         | running | n/a | mysql-partition-3333e3ee3332221e222e                         | 10.0.16.18 |
| mysql_proxy-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-hhh111111111)                   | running | n/a | mysql_proxy-partition-3333e3ee3332221e222e                   | 10.0.16.17 |
| nats-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-iii111111111)                          | running | n/a | nats-partition-3333e3ee3332221e222e                          | 10.0.16.12 |
| nfs_server-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-jjj111111111)                    | running | n/a | nfs_server-partition-3333e3ee3332221e222e                    | 10.0.16.15 |
| router-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-kkk111111111)                        | running | n/a | router-partition-3333e3ee3332221e222e                        | 10.0.16.16 |
| uaa-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-lll111111111)                           | running | n/a | uaa-partition-3333e3ee3332221e222e                           | 10.0.16.22 |
+-------------------------------------------------------------------------------------------------------+---------+-----+--------------------------------------------------------------+------------+
VMs total: 18
</pre>

When troubleshooting an issue with your deployment, `bosh vms` may show a VM in
an **unknown** state.
Run [bosh cloudcheck](#cck) on a VM in an **unknown** state to instruct BOSH to
diagnose problems with the VM.

You can also run `bosh vms` to identify VMs in your deployment, then use the
[bosh ssh](#ssh) command to SSH into an identified VM for further
troubleshooting.

`bosh vms` supports the following arguments:

* `--details`: Report also includes Cloud ID, Agent ID, and whether or not the BOSH Resurrector has been enabled for each VM

* `--vitals`: Report also includes load, CPU, memory usage, swap usage, system disk usage, ephemeral disk usage, and persistent disk usage for each VM

* `--dns`: Report also includes the DNS A record for each VM

<p class="note"><strong>Note</strong>: The <strong>Status</strong> tab of the <strong>Elastic Runtime</strong> product tile displays information similar to the <code>bosh vms</code> output.</p>

### <a id='cck'></a> BOSH Cloud Check

Run the `bosh cloudcheck` command to instruct BOSH to detect differences
between the VM state database maintained by the BOSH Director and the actual
state of the VMs. For each difference detected, `bosh cloudcheck` can offer the
following repair options:

* `Reboot VM`: Instructs BOSH to reboot a VM. Rebooting can resolve many
transient errors.
* `Ignore problem`: Instructs BOSH to do nothing.
You may want to ignore a problem in order to run `bosh ssh` and attempt
troubleshooting directly on the machine.
* `Reassociate VM with corresponding instance`: Updates the BOSH Director state
database.
Use this option if you believe that the BOSH Director state database is in
error and that a VM is correctly associated with a job.
* `Recreate VM using last known apply spec`: Instructs BOSH to destroy the
server and recreate it from the deployment manifest that the installer
provides. Use this option if a VM is corrupted.
* `Delete VM reference`: Instructs BOSH to delete a VM reference in the
Director state database. If a VM reference exists in the state database, BOSH
expects to find an agent running on the VM.
Select this option only if you know that this reference is in error.
Once you delete the VM reference, BOSH can no longer control the VM.

#### Example Scenarios

**Unresponsive Agent**

<pre class='terminal'>
$ bosh cloudcheck
ccdb/0 (vm-3e37133c-bc33-450e-98b1-f86d5b63502a) is not responding:

- Ignore problem
- Reboot VM
- Recreate VM using last known apply spec
- Delete VM reference (DANGEROUS!)
</pre>

**Missing VM**

<pre class='terminal'>
$ bosh cloudcheck
VM with cloud ID `vm-3e37133c-bc33-450e-98b1-f86d5b63502a' missing:

- Ignore problem
- Recreate VM using last known apply spec
- Delete VM reference (DANGEROUS!)
</pre>

**Unbound Instance VM**

<pre class='terminal'>
$ bosh cloudcheck
VM `vm-3e37133c-bc33-450e-98b1-f86d5b63502a' reports itself as `ccdb/0' but does not have a bound instance:

- Ignore problem
- Delete VM (unless it has persistent disk)
- Reassociate VM with corresponding instance
</pre>

**Out of Sync VM**

<pre class='terminal'>
$ bosh cloudcheck
VM `vm-3e37133c-bc33-450e-98b1-f86d5b63502a' is out of sync:
expected `cf-d7293430724a2c421061: ccdb/0', got `cf-d7293430724a2c421061: nats/0':

- Ignore problem
- Delete VM (unless it has persistent disk)
</pre>

### <a id='bosh-ssh'></a> BOSH SSH

Use `bosh ssh` to SSH into the VMs in your deployment.

Follows the steps below to use `bosh ssh`:

1. Run `ssh-keygen -t rsa` to provide BOSH with the correct public key.

1. Accept the defaults.

1. Run `bosh ssh`.

1. Select a VM to access.

1. Create a password for the temporary user that the `bosh ssh` command creates.
Use this password if you need sudo access in this session.

Example:

<pre class='terminal'>
$ bosh ssh
RSA 1024 bit CA certificates are loaded due to old openssl compatibility
1. diego_brain-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc555555555)
2. uaa-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-lll111111111)
3. cloud_controller_worker-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc333333333)
4. cloud_controller-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc222222222)
5. diego_cell-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc666666666)
6. diego_cell-partition-3333e3ee3332221e222e/1 (abc31111-111e-1ec3-bb3e-ccc777777777)
7. diego_cell-partition-3333e3ee3332221e222e/2 (abc31111-111e-1ec3-bb3e-ccc888888888)
8. router-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-kkk111111111)
9. loggregator_trafficcontroller-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-fff223111111)
10. nats-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-iii111111111) 
11. clock_global-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc111111111)
12. mysql_proxy-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-hhh111111111)
13. diego_database-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc999999999)
14. etcd_server-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-eee111111111)
15. mysql-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ggg111111111)
16. consul_server-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ccc444444444)
17. doppler-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-ddd111111111)
18. nfs_server-partition-3333e3ee3332221e222e/0 (abc31111-111e-1ec3-bb3e-jjj111111111)
Choose an instance:

Choose an instance: 5
Enter password (use it to sudo on remote host): *******
Target deployment 'cf-33e333333eebbb3b33b3'

Setting up ssh artifacts
</pre>

### <a id='standard-ssh'></a> Standard SSH

In most cases, operators should use the `bosh ssh` command in the BOSH CLI to SSH into the BOSH Director and other VMs in their deployment. However, operators can also use standard `ssh` by performing the procedures below.

1. Locate the IP address of your BOSH Director and your BOSH Director credentials by following the [steps](#gather) above.

1. SSH into the BOSH Director with the private key you used with `bosh-init` to deploy the BOSH Director:
<pre class="terminal">$ ssh BOSH-DIRECTOR-IP -i PATH-TO-PRIVATE-KEY </pre>

1. Enter your BOSH Director credentials to log in.

From the BOSH Director, you can SSH into the other VMs in your deployment by performing the following steps:

1. Identify the private IP address of the component VM you want to SSH into by doing one of the following:
    * Perform the [steps](#prepare) above to use the BOSH CLI to log in to your BOSH Director and use `bosh vms` to [list](#vms) the IP addresses of your component VMs.
    * Navigate to your IaaS console and locate the IP address of the VM. For example, Amazon Web Services users can locate the IP addresses of component VMs in the VPC Dashboard of the AWS Console.
1. SSH into the component VM:
<pre class="terminal">$ ssh COMPONENT-VM-PRIVATE-IP</pre>