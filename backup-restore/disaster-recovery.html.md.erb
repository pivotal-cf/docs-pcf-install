---
title: Disaster Recovery in Pivotal Cloud Foundry
owner: BBR
---

This document provides an overview of the options and considerations for disaster recovery in <%= vars.first_product_name  %>.

Operators have a range of approaches for ensuring they can recover Pivotal Cloud Foundry, apps, and data in case of a disaster. The approaches fall into the following two categories:

* Using data from a backup to restore the data in the PCF Deployment. See [Back up and Restore Using BOSH Backup and Restore (BBR)](#bbr) for more information.
* Re-creating the data in PCF by automating the creation of state in PCF. See [Disaster Recovery by Recreating the Deployment](#recreation) for more information.

## <a id="bbr"></a>Back up and Restore using BOSH Backup and Restore (BBR)

### <a id="what-is-bbr"></a>What is BBR?

BOSH Backup and Restore (BBR) is a CLI for orchestrating backing up and restoring BOSH deployments and BOSH Directors. BBR triggers the backup or restore process on the deployment or Director, and transfers the backup artifact to and from the deployment or Director.

Use BOSH Backup and Restore to reliably create backups of core PCF components and their data. These core components include CredHub, UAA, BOSH Director, and <%= vars.product_short %>.

Each component includes its own backup scripts. This decentralized structure helps keep scripts synchronized with the components. At the same time, _locking_ features ensure data integrity and consistent, distributed backups across your deployment.

For more information about the BBR framework, see [BOSH Backup and Restore](https://docs.cloudfoundry.org/bbr/index.html) in the open source Cloud Foundry documentation.

### <a id="bbr-backup"></a>Backing up PCF

Backing up PCF requires backing up the following components:

* Ops Manager settings
* BOSH Director, including CredHub and UAA
* <%= vars.product_full %>
* Data services

For more information, see [Backing up Pivotal Cloud Foundry with BBR](./backup-pcf-bbr.html). With these backup artifacts, operators can re-create PCF exactly as it was when the backup was taken.

### <a id="bbr-restore"></a> Restoring PCF

The restore process involves creating a new PCF deployment starting with the Ops Manager VM. For more information, see [Restoring Pivotal Cloud Foundry from Backup with BBR](./restore-pcf-bbr.html).

The time required to restore the data is proportionate to the size of the data because the restore process includes copying data. For example, restoring a 1&nbsp;TB blobstore takes one thousand times as long as restoring a 1&nbsp;GB blobstore.

### <a id="benefits"></a> Benefits

Unlike other backup solutions, using BBR to back up PCF enables the following:

* **Completeness**: BBR supports backing up BOSH, including releases, CredHub, UAA, and service instances created with an on-demand service broker. With PCF v1.12, Ops Manager export no longer includes releases.
* **Consistency**: BBR provides referential integrity between the database and the blobstore because a lock is held while both the database and blobstore are backed up.
* **Correctness**: Using the BBR restore flow addresses C2C and routing issues that can occur during restore.

### <a id="api-downtime"></a> API Downtime During Backups

Apps are not affected during backups, but certain APIs are unavailable. The downtime occurs only while the backup is being taken, not while the backup is being copied to the jumpbox.

In a consistent backup, the blobs in the blobstore match the blobs in the Cloud Controller database. To take a consistent backup, changes to the data are prevented during the backup. This means that the CF API, Routing API, Usage service, Autoscaler, Notification Service, Network Policy Server, and CredHub are unavailable while the backup is being taken. UAA is in read-only mode during the backup.

### <a id="backup-timings"></a> Backup Timings
The first three phases of the backup are lock, backup, and unlock. During this time, the API is unavailable. The drain and checksum phase starts after the backup scripts finish. BBR downloads the backup artifacts from the instances to the BBR VM, and performs a checksum to ensure the artifacts are not corrupt. The size of the blobstore significantly influences backup time.

The table below gives an indication of the downtime that you can expect. Actual downtime varies based on hardware and PCF configuration. These example timings were recorded with Pivotal Application Service (PAS) deployed on Google Cloud Platform (GCP) with all components scaled to one and only one app pushed.

<table>
  <tr>
    <th colspan="5"><strong>Backup Timings</strong></th>
  </tr>
  <tr>
   <td><strong>API State</strong></td>
   <td><strong>Backup phase</strong></td>
   <td><strong>Duration for External Versioned S3-Compatible Blobstore</strong></td>
   <td><strong>Duration for External Unversioned S3-Compatible Blobstore</strong></td>
   <td><strong>Duration for Internal Blobstore</strong></td>
   </tr>
  <tr>
    <td rowspan="3">API unavailable</td>
    <td>lock</td>
    <td>15 seconds</td>
    <td>15 seconds</td>
    <td>15 seconds</td>
  </tr>
  <tr>
    <td>backup</td>
    <td><30 seconds</td>
     <td>Proportional to blobstore size</td>
    <td>10 seconds</td>
  </tr>
  <tr>
    <td>unlock</td>
    <td>3 minutes</td>
    <td>3 minutes</td>
    <td>3 minutes</td>
  </tr>
  <tr>
    <td>API available</td>
    <td>drain and checksum</td>
    <td><10 seconds</td>
    <td><10 seconds</td>
    <td>Proportional to blobstore size</td>
  </tr>
</table>

#### <a id="blobstore-bbr"></a>Blobstore Backup and Restore

Blobstores can be very large. To minimize downtime, BBR only takes blob metadata during the backup. For example, in the case of internal blobstores (Webdav/NFS), BBR takes a list of hard links that point to the blobs. After the API becomes available, BBR makes copies of the blobs.

### <a id="not-supported"></a>Unsupported Products

* **Data services**. The Pivotal data services listed below do not support BBR. Operators
of these services should use the automatic backups feature of each tile, available
within Ops Manager.

  * MySQL for PCF
  * Pivotal Cloud Cache for PCF
  * RabbitMQ for PCF
  * Redis for PCF

* **External blobstores and databases**. BBR support for backing up and restoring external
databases and blobstores varies across PCF versions. For more information, see
[Supported Components](backup-pcf-bbr.html#supported) and
[External Storage Support Across PCF Versions](backup-pcf-bbr.html#version-compatibility)
in _Backing up Pivotal Cloud Foundry with BBR_.

### <a id="best-practices"></a> Best Practices

#### <a id="backup-frequency"></a> Frequency of Backups

Pivotal recommends that you take backups in proportion to the rate of change of the data in PCF to minimize the number of changes lost if a restore is required. We suggest starting with backing up every 24 hours. If app developers make frequent changes, you should increase the frequency of backups.

#### <a id="artifact-retention"></a> Retention of Backup Artifacts

Operators should retain backup artifacts based on the timeframe they need to be able to restore to. For example, if backups are taken every 24 hours and PCF must be able to be restored to three days prior, three sets of backup artifacts should be retained.

Artifacts should be stored in two data centers other than the PCF data center.
When deciding the restore timeframe, you should take other factors such as compliance and audit-ability into account.

#### <a id="security"></a>Security

Pivotal recommends that you encrypt artifacts and store them securely.

## <a id="recreation"></a> Disaster Recovery by Re-creating the Deployment

An alternative strategy for recovering PCF after a disaster is to have automation in place so that all the data can be re-created. This requires that every modification to PCF settings and state is automated, typically through the use of a pipeline.

Recovery steps include creating a new PCF, re-creating orgs, spaces, users, services, service bindings and other state, and re-pushing apps.

For more information about this approach, see the Cloud Foundry Summit presentation [Multi-DC Cloud Foundry: What, Why and How?](https://www.youtube.com/watch?v=t61r8THmtsc).

## <a id="different-topologies"></a> Disaster Recovery for Different Topologies

### <a id="active-active"></a>  Active-Active

To prevent app downtime, some Pivotal customers run active-active, where they run two or more identical PCF deployments in different data centers. If one PCF deployment becomes unavailable, traffic is seamlessly routed to the other deployment. To achieve identical deployments, all operations to PCF are automated so they can be applied to both PCF deployments in parallel.

Because all operations have been automated, the automation approach to disaster recovery is a viable option for active-active. Disaster recovery requires re-creating PCF, then running all the automation to re-create state.

This option requires discipline to automate all changes to PCF. Some of the operations that need to be automated are the following:

* App push, restage, scale
* Org, space, and user create, read, update, and delete (CRUD)
* Service instance CRUD
* Service bindings CRUD
* Routes CRUD
* Security groups CRUD
* Quota CRUD

Human-initiated changes always make their way into the system. These changes can include quotas being raised, new settings being enabled, and incident responses. For this reason, Pivotal recommends taking backups even when using an automated disaster recovery strategy.

#### <a id="bbr-aa-comparison"></a> Using BBR Backup and Restore versus Recreating a Failed PCF Deployment in Active-Active

<table>
<tr>
<th colspan="3">
Disaster Recovery
</th>
</tr>
<tr>
<th>
</th>
<th>
Restore the PCF Data
</th>
<th>
Re-create the PCF Data
</th>
</tr>
<tr>
<th>Preconditions</th>
<td>
IaaS prepared for PCF install
</td>
<td>
IaaS prepared for PCF install
</td>
</tr>
<tr>
<th>
Steps
</th>
<td>
<ol>
<li>Re-create PCF</li>
<li>Restore </li>
<li>Apply changes to make restored PCF match the other active PCF</li>
</ol>
</td>
<td>
<ol>
<li>Re-create PCF</li>
<li>Trigger automation to re-create orgs, spaces, etc.</li>
<li>Notify app developers to re-push apps, re-create service instances and bindings</li>
</ol>
</td>
</tr>
<tr>
<th colspan="3">
RTO (Recovery Time Objective)
</th>
</tr>
<tr>
<th>
Platform
</th>
<td>
Time to re-create PCF
</td>
<td>
Time to re-create PCF
</td>
</tr>
<tr>
<th>
Apps
</th>
<td>
Time to restore
</td>
<td>
Time until orgs/spaces/etc have been re-created and apps have been re-pushed
</td>
</tr>
<tr>
<th colspan="3">
RPO (Recovery Point Objective)
</th>
</tr>
<tr>
<th>
Platform
</th>
<td>
Time of the last backup
</td>
<td>
Current time
</td>
</tr>
<tr>
<th>
Apps
</th>
<td>
Time of the last backup
</td>
<td>
Current time
</td>
</tr>
</table>

### <a id="active-passive"></a>  Active-Passive

Instead of having a true active-active deployment across all layers, some Pivotal customers prefer to install a PCF or <%= vars.product_short %> deployment on a backup site. The backup site resides on-premises, in a co-location facility, or the public cloud. The backup site includes an operational deployment, with only the most critical apps ready to accept traffic should a failure occur in the primary data center. Disaster recovery in this scenario involves the following:

1. Switching traffic to the passive PCF, making it active.
1. Recovering the formerly-active PCF. Operators can choose to do this through automation, if that option is available, or by using BBR and the restore process.

The RTO and RPO for re-creating the active PCF are the same as outlined in the table above.

## <a id="reducing-rto"></a> Reducing RTO

Both the restore and re-create data disaster recovery options require standing up a new PCF, which can take hours. If you require shorter RTO, several options involving a pre-created standby hardware and PCF are available:

<table>
<tbody>
<tr>
<th>
<p><strong>Active-cold</strong></p>
</th>
<td>
<p>Public cloud environment ready for PCF installation, no PCF installed. This saves both IaaS costs and PCF instance costs. For on-on-premise installations, this requires hardware on standby, ready to install on, which might not be a realistic option.</p>
</td>
</tr>
<tr>
<th>
<p><strong>Active-warm</strong></p>
</th>
<td>
<p>PCF installed on standby hardware and kept up to date, VMs scaled down to zero (spin them up each time there is a platform update), no apps installed, no orgs or spaces defined.</p>
</td>
</tr>
<tr>
<th>
<p><strong>Active-inflate platform</strong></p>
</th>
<td>
<p>Bare minimum PCF installation, either with no apps, or a small number of each app in a stopped state. On recovery, push a small number of apps or start current apps, while simultaneously triggering automation to scale the platform to the primary node size, or a smaller version if large percentages of loss are acceptable. This mode allows you to start sending some traffic immediately, while not paying for a full non-primary platform. This method requires data seeded, but it is usually acceptable to complete data sync while platform is scaling up.</p>
</td>
</tr>
<tr>
<th>
<p><strong>Active-inflate apps</strong></p>
</th>
<td>
<p>Non-primary deployment scaled to the primary node size, or smaller version if large percentages of loss are acceptable, with a small number of Diego Cells (VMs). On fail-over, scale Diego Cells up to primary node counts. This mode allows you to start sending most traffic immediately, while not paying for all the AIs of a fully fledged node. This method requires data to be there very quickly after failure. It does not require real-time sync, but near-real time. </p>
</td>
</tr>
</tbody>
</table>

There is a trade-off between cost and RTO: the less the replacement PCF needs to be deployed and scaled, the faster the restore.

## <a id="automating-backups"></a> Automating Backups

BBR generates the backup artifacts required for PCF, but does not handle scheduling, artifact management, or encryption. The BBR team has created a [starter Concourse pipeline](https://github.com/pivotal-cf/bbr-pcf-pipeline-tasks) to automate backups with BBR.

Also, Stark & Wayne's Shield can be used as a front-end management tool using the BBR plugin.

## <a id="validating-backups"></a> Validating Backups

To ensure that backup artifacts are valid, the BBR tool creates checksums of the generated backup artifacts, and ensures that the checksums match the artifacts on the jumpbox.

However, the only way to be sure that the backup artifact can be used to successfully re-create PCF is to test it in the restore process. This is a cumbersome, dangerous process so should be done with care. For instructions, see [Step 11: (Optional) Validate Your Backup](./backup-pcf-bbr.html#validate-backup) in the Backing Up Pivotal Cloud Foundry with BBR topic.
