---
title: Restoring PCF from Backup with BBR
owner: RelEng
---

This topic describes the procedure for restoring your critical back end <%= vars.product_name_full %> (<%= vars.product_name %>) components with BOSH Backup and Restore (BBR), a command-line tool for backing up and restoring BOSH deployments. To perform the procedures in this topic, you must have backed up <%= vars.product_name %> by following the procedure in [Backing Up <%= vars.product_name %> with BBR](backup-pcf-bbr.html).

For BBR release notes, see [BOSH Backup and Restore Release Notes](https://docs.cloudfoundry.org/bbr/bbr-rn.html) in the Cloud Foundry documentation.


## <a id='overview'></a> Overview

The procedures described in this topic prepare your environment for <%= vars.product_name %>, deploy <%= vars.ops_manager_full %>, import your installation settings, and use BBR to restore your <%= vars.product_name %> components.

<p class="note warning"><strong>Warning:</strong> Restoring <%= vars.product_name %> with BBR is a destructive operation. If the restore fails, the new environment may be left in an unusable state and require re-provisioning. Only perform the procedures in this topic for the purpose of disaster recovery, such as recreating <%= vars.product_name %> after a storage-area network (SAN) corruption.</p>

<p class="note warning"><strong>Warning:</strong> When validating your backup, the VMs and disks from the backed-up BOSH Director should not be visible to the new BOSH Director. As a result, <%= vars.recommended_by %> recommends that you deploy the new BOSH Director to a different IaaS network and account than the VMs and disks of the backed-up BOSH Director.</p>

<p class="note"><strong>Note:</strong> If the BOSH Director you are restoring had any deployments that were deployed manually rather than through a <%= vars.ops_manager %> tile, you must restore them manually at the end of the process. For more information, see <a href="#non-tiles">(Optional) Restore Non-Tile Deployments</a>.</p>


## <a id="compatibility"></a> Compatibility of Restore

This section describes the restrictions for a backup artifact to be restorable to another environment. This section is for guidance only, and <%= vars.recommended_by %> recommends that you validate your backups by using the backup artifacts in a restore.

For a backup artifact to be restorable, these restrictions apply:

* **CIDR ranges**: BBR requires the IP address ranges to be the same in the restore environment as in the backup environment.

* **Topology**: BBR requires the BOSH topology of a deployment to be the same in the restore environment as it was in the backup environment.

* **Naming of instance groups and jobs**: For any deployment that implements the backup and restore scripts, the instance groups and jobs must have the same names.

* **Number of instance groups and jobs**: For instance groups and jobs that have backup and restore scripts, there must be the same number of instances.

* **Limited validation**: BBR puts the backed-up data into the corresponding instance groups and jobs in the restored environment, but cannot validate the restore beyond that. For example, if the MySQL encryption key is different in the restore environment, the BBR restore might succeed, but the restored MySQL database is unusable.

* **<%= vars.product_name %> version**: BBR can restore to the same version of <%= vars.product_name %> that was backed up. BBR does not support restoring to other major, minor, or patch releases.

<p class="note"><strong>Note:</strong> A change in VM size or underlying hardware will not affect BBR's ability to restore data, as long as there is adequate storage space to restore the data.</p>


## <a id="restore-diagram"></a> Restore Workflow

The diagram below illustrates the flow of the <%= vars.product_name %> restore process in a series of steps performed by the <%= vars.product_name %> operator. The following steps are covered in more detail throughout this topic.

![Flow chart shows the platform Operator through a series of steps interacting with the BOSH CLI, <%= vars.ops_manager %> VM, BOSH Director VM, and the <%= vars.product_short %> tile. Details on these steps are described below.](./images/pcf-restore.png)

[View a larger version of this diagram](./images/pcf-restore.png).

1. **Launch new <%= vars.ops_manager %>**: Follow the procedures for your IaaS to deploy <%= vars.ops_manager %>. For more information, see Step 1 of [Deploy <%= vars.ops_manager %> and Import Installation Settings](#deploy-import).

1. **Import settings**: You can import settings either with the <%= vars.ops_manager %> UI or API. For more information, see Step 2 of [Deploy <%= vars.ops_manager %> and Import Installation Settings](#deploy-import).

1. **Remove bosh-state.json**: SSH into your <%= vars.ops_manager %> VM and delete the `bosh-state.json` file. For more information, see [Remove BOSH State File](#bosh-state).

1. **Apply Changes (BOSH Director only)**: Use the <%= vars.ops_manager %> API to only deploy the BOSH Director. For more information, see [Deploy the BOSH Director](#bosh-only-deploy).

1. **bbr restore &lt;director&gt;**: Run the BBR restore command from your jumpbox to restore the BOSH Director. For more information, see [Restore the BOSH Director](#restore-bosh).

1. **Use BOSH cck to fix the stale cloud ID references in the BOSH database**: For each deployment in the BOSH Director, you must run a `bosh cloud-check` command. For more information, see [Remove Stale Cloud IDs for All Deployments](#bosh-cck).

1. **Apply Changes**: In the <%= vars.ops_manager %> Installation Dashboard, click **Review Pending Changes**, review your changes, and click **Apply Changes**. For more information, see [Reviewing Pending Product Changes](../review-pending-changes.html).

1. **bbr restore &lt;<%= vars.product_short %>&gt;**: Run the BBR restore command from your jumpbox to restore <%= vars.product_runtime %> (<%= vars.product_short %>). For more information, see [Restore <%= vars.product_short %>](#restore-ert).


## <a id='prepare-restore'></a> Prepare to Restore

These sections describe the procedure you must perform before restoring your <%= vars.product_name %> backup with BBR.

### <a id='prepare-env'></a> Step 1: (Optional) Prepare Your Environment

In the event of a disaster, you can lose not only your VMs and disks, but your IaaS resources as well, such as networks and load balancers.

If you need to recreate your IaaS resources, prepare your environment for <%= vars.product_name %> by following the instructions specific to your IaaS in [Installing Pivotal Cloud Foundry](../../installing/index.html).

<p class="note"><strong>Note:</strong> The instructions for installing <%= vars.product_name %> on Amazon Web Services (AWS) and OpenStack combine the procedures for preparing your environment and deploying <%= vars.ops_manager %> into a single topic. The instructions for the other supported IaaSes split these procedures into two separate topics.</p>

If you re-create your IaaS resources, you must also add those resources to <%= vars.ops_manager %> by following the procedures in [Step 3: (Optional) Configure <%= vars.ops_manager %> for New Resources](#config-new-resources).

### <a id='deploy-import'></a> Step 2: Deploy <%= vars.ops_manager %> and Import Installation Settings

1. Follow the procedures for your IaaS to deploy <%= vars.ops_manager %>:
    * **AWS**:
        * If you configured AWS manually, see [Deploying <%= vars.ops_manager %> on AWS Manually](/platform/<%= vars.current_major_version.sub('.', '-') %>/om/aws/deploy-manual.html).
        * If you used Terraform to install <%= vars.product_name %> on AWS, see [Installing <%= vars.product_name %> on AWS Using Terraform](../aws-terraform.html).
    * **GCP**: See [Deploying <%= vars.ops_manager %> on GCP Manually](/platform/<%= vars.current_major_version.sub('.', '-') %>/om/gcp/deploy-manual.html).
    * **OpenStack**: See _Step 4: Create <%= vars.ops_manager %> Image_ through _Step 9: Create a DNS Entry_ in [Deploying <%= vars.ops_manager %> on OpenStack](/platform/<%= vars.current_major_version.sub('.', '-') %>/om/openstack/setup.html#create-om-image).
    * **vSphere**: See [Deploying <%= vars.ops_manager %> on vSphere](/platform/<%= vars.current_major_version.sub('.', '-') %>/om/vsphere/deploy.html).

1. Import your installation settings. This can be done in two ways:
    1. Using the <%= vars.ops_manager %> UI:
        1. Access your new <%= vars.ops_manager %> by navigating to your Ops Manager fully-qualified domain name (FQDN) in a browser.
        1. On the **Welcome to <%= vars.ops_manager %>** page, click **Import Existing Installation**.

            <%= image_tag("../images/upgrading/welcome.png") %>

        1. In the import panel:
          * Enter the **Decryption Passphrase** in use when you exported the installation settings from <%= vars.ops_manager %>.
          * Click **Choose File** and browse to the installation ZIP file that you exported in [Step 9: Export Installation Settings](./backup-pcf-bbr.html#export) in _Backing Up <%= vars.product_name %> with BBR_.

            <%= image_tag("../images/upgrading/decryption_passphrase.png") %>

        1. Click **Import**.
            <p class="note"><strong>Note:</strong> Some browsers do not provide feedback on the status of the import process, and might appear to hang. The import process takes at least 10 minutes, and takes longer the more tiles that were present on the backed-up <%= vars.ops_manager %>.</p>

        1. A **Successfully imported installation** message appears upon completion.

            <%= image_tag("../images/upgrading/success.png") %>

    1. Using the <%= vars.ops_manager %> API:
        1. Run:

            ```
            curl "https://OPS-MANAGER-FQDN/api/v1/installation_asset_collection \
              -X POST \
              -H "Authorization: Bearer UAA-ACCESS-TOKEN" \
              -F 'installation[file]=@installation.zip' \
              -F 'passphrase=DECRYPTION-PASSPHRASE'
            ```
            Where:
            * `OPS-MANAGER-FQDN` is the FQDN for your <%= vars.ops_manager %> deployment.
            * `UAA-ACCESS-TOKEN` is the UAA access token. For more information about how to retrieve this token, see [Using the <%= vars.ops_manager %> API](../../customizing/ops-man-api.html).
            * `DECRYPTION-PASSPHRASE` is the decryption passphrase in use when you exported the installation settings from <%= vars.ops_manager %>.

<p class="note warning"><strong>Warning:</strong> Do not click <strong>Apply Changes</strong> in <%= vars.ops_manager %> until the instruction in <em>Step 14: Redeploy PAS</em>.</p>

### <a id="config-new-resources"></a> Step 3: (Optional) Configure <%= vars.ops_manager %> for New Resources

If you re-created IaaS resources such as networks and load balancers by following the procedure in [Step 1: (Optional) Prepare Your Environment](#prepare-env), you must update <%= vars.ops_manager %> with your new resources.'

To update <%= vars.ops_manager %>:

1. Enable the advanced mode of <%= vars.ops_manager %>. For more information, see [How to Enable Advanced Mode in the Ops Manager](https://community.pivotal.io/s/article/How-to-Enable-Advanced-Mode-in-the-Ops-Manager) in the Pivotal Knowledge Base.
    <p class="note"><strong>Note:</strong> In advanced mode, <%= vars.ops_manager %> allows you to make changes that are normally disabled. You might see warning messages when you save changes.</p>

1. Navigate to the <%= vars.ops_manager %> Installation Dashboard.

1. Click the BOSH Director tile.

1. If you are using Google Cloud Platform (GCP), click **Google Config** and update:
    1. **Project ID** to reflect the GCP project ID.
    1. **Default Deployment Tag** to reflect the environment name.
    1. **AuthJSON** to reflect the service account.

1. Click **Create Networks** and update the network names to reflect the network names for the new environment.

1. If your BOSH Director had an external hostname, modify the **Director Hostname** field in the **Director Config** pane of the BOSH Director tile to ensure it does not conflict with the hostname of the backed-up Director.

1. Return to the <%= vars.ops_manager %> Installation Dashboard.

1. Click the <%= vars.product_short %> tile.

1. Select **Resource Config**.

1. If necessary for your IaaS, enter the name of your new load balancers in the **Load Balancers** column.

1. If necessary, click **Networking** and update the load balancer SSL certificate and private key under **Certificates and Private Keys for HAProxy and Router**.

1. If your environment has a new DNS address, update the old environment DNS entries to point to the new load balancer addresses. For more information, see [Step 4: Configure Networking](../custom-load-balancer.html#configure-networking) in _Using Your Own Load Balancer_ and follow the link to the procedure for your IaaS.

1. If your <%= vars.product_short %> deployment uses an external blobstore, ensure that the <%= vars.product_short %> tile is configured to use a different blobstore. Otherwise, it attempts to connect to the blobstore that the backed-up <%= vars.product_short %> deployment is using.

1. Select **Domains**.

1. Update your **System Domain** and **Apps Domain** to refer to the new environment's domains.

1. Ensure that there are no outstanding warning messages in the BOSH Director tile, then disable <%= vars.ops_manager %> advanced mode. For more information, see [How to Enable Advanced Mode in the Ops Manager](https://community.pivotal.io/s/article/How-to-Enable-Advanced-Mode-in-the-Ops-Manager) in the Pivotal Knowledge Base.

### <a id="bosh-state"></a> Step 4: Remove BOSH State File

To remove the BOSH state file:

1. SSH into your <%= vars.ops_manager %> VM. For more information, see [SSH Into <%= vars.ops_manager %>](../trouble-advanced.html#ssh) in _Advanced Troubleshooting with the BOSH CLI_.

1. To delete the `/var/tempest/workspaces/default/deployments/bosh-state.json` file, go to the <%= vars.ops_manager %> VM and run:

    ```
    sudo rm /var/tempest/workspaces/default/deployments/bosh-state.json
    ```

1. In a browser, navigate to <%= vars.ops_manager %>'s FQDN.

1. Log in to <%= vars.ops_manager %>.

### <a id="bosh-only-deploy"></a> Step 5: Deploy the BOSH Director

Use the <%= vars.ops_manager %> API or the checkbox on the **Review Pending Changes** page to deploy the BOSH Director by itself.

### <a id="artifacts-jumpbox"></a> Step 6: Transfer Artifacts to Jumpbox

In [Step 9: Back Up Your <%= vars.product_short %> Deployment](./backup-pcf-bbr.html#bbr-backup) in _Backing Up <%= vars.product_name %> with BBR_, in the _After Taking the Backups_ section, you moved the TAR and metadata files of the backup artifacts off your jumpbox to your preferred storage space. Now you must transfer those files back to your jumpbox.


## <a id='restore'></a> Restore Your Backup

These sections describe the procedure to restore your <%= vars.product_name %> backup with BBR.

### <a id='retrieve-credentials'></a> Step 7: Retrieve BOSH Director Credentials

<%= partial 'bosh_retrieve_credentials' %>

### <a id="retrieve"></a> Step 8: Retrieve BOSH Director Address

To retrieve the IP address of your BOSH Director from the BOSH Director tile:

<%= partial 'bosh_target_director_bbr' %>

### <a id='restore-bosh'></a> Step 9: Restore BOSH Director

<div class="note"><strong>Notes:</strong>
  <ul>
    <li>The BBR BOSH Director restore command can take at least 15 minutes to complete.</li>
    <li><%= vars.recommended_by %> recommends that you run it independently of the SSH session, so that the process can continue running even if your connection to the jumpbox fails. The command above uses <code>nohup</code> but you could also run the command in a <code>screen</code> or <code>tmux</code> session.</li>
  </ul>
</div>

To restore BOSH Director:

1. SSH into your jumpbox. If you are using the <%= vars.ops_manager %> VM as your jumpbox, see [Log In to the <%= vars.ops_manager %> VM with SSH](../trouble-advanced.html#ssh) in _Advanced Troubleshooting with the BOSH CLI_ for procedures on how to use SSH to connect to the <%= vars.ops_manager %> VM.

1. Ensure the BOSH Director backup artifact is in the folder you from which you will run BBR.

1. Run the BBR restore command from your jumpbox to restore the BOSH Director:

    ```
    bbr director \
    --private-key-path PRIVATE-KEY-FILE \
    --username bbr \
    --host HOST \
    restore \
    --artifact-path PATH-TO-DIRECTOR-BACKUP
    ```
    Where:
    <br>
    * `PATH-TO-DIRECTOR-BACKUP` is the path to the BOSH Director backup you want to restore.
    * `PRIVATE-KEY-FILE` is the path to the private key file you created in [Step 7: Retrieve BOSH Director Credentials](#retrieve-credentials).
    * `HOST` is the address of the BOSH Director.
      * If the BOSH Director is public, `HOST` is a URL, such as `https://bosh-director.xxx.cf-app.com`.
      * If the BOSH Director is not public, `HOST` is the `BOSH-DIRECTOR-IP` retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * (Optional) Use the `--debug` flag to enable debug logs. For more information, see [Logging](backup-pcf-bbr.html#logging) in _Backing Up <%= vars.product_name %> with BBR_.

1. After the command succeeds, continue to [Step 10: Identify Your Deployment](#identify-deployment). If the command fails:
  1. Run:

        ```
        bbr director \
        --private-key-path PRIVATE-KEY-FILE \
        --username bbr \
        --host HOST \
        restore-cleanup
        ```
        Where:
        <br>
        * `PRIVATE-KEY-FILE`is the path to the private key file you created in [Step 7: Retrieve BOSH Credentials](#retrieve-credentials).
        * `HOST` is the address of the BOSH Director.
          * If the BOSH Director is public, `HOST` is a URL, such as `https://bosh-director.xxx.cf-app.com`.
          * If the BOSH Director is not public, `HOST` is the `BOSH-DIRECTOR-IP` retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
  1. Run the BBR restore command again after ensuring that:
        * All the parameters in the command are set.
        * The BOSH Director credentials are valid.
        * The specified deployment exists.
        * The source deployment is compatible with the target deployment.
        * That the jumpbox can reach the BOSH Director.

### <a id='identify-deployment'></a> Step 10: Identify Your Deployment

To identify the name of the BOSH deployment that contains <%= vars.product_name %>:

1. Log in to your BOSH Director.

1. Run:

    ```
    bosh -e BOSH-DIRECTOR-IP --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE deployments
    ```
    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the Certificate Authority (CA) certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.

    For example:
    <pre class='terminal'>
    $ bosh -e BOSH-DIRECTOR-IP --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE deployments
    <br>
    Name                     Release(s)
    cf-example               push-apps-manager-release/661.1.24
                                 cf-backup-and-restore/0.0.1
                                 binary-buildpack/1.0.11
                                 capi/1.28.0
                                 cf-autoscaling/91
                                 cf-mysql/35
                                 ...
    </pre>
    <br>
    In the above example, the name of the BOSH deployment that contains <%= vars.product_name %> is `cf-example`. `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the root CA certificate for the BOSH Director. If you are using the <%= vars.ops_manager %> VM as your jumpbox, the path is `/var/tempest/workspaces/default/root_ca_certificate`.

### <a id="bosh-cck"></a> Step 11: Remove Stale Cloud IDs for All Deployments

To remove stale cloud IDs for all deployments:

1. Review the deployments listed when performing [Step 10: Identify Your Deployment](#identify-deployment).

1. To reconcile the BOSH Director's internal state with the state in the IaaS, run:

    ```
    bosh -e BOSH-DIRECTOR-IP \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    -d DEPLOYMENT-NAME -n cck \
    --resolution delete_disk_reference \
    --resolution delete_vm_reference
    ```

    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.
    * `DEPLOYMENT-NAME` is the deployment name retrieved in [Step 10: Identify Your Deployment](#identify-deployment).
    <br>
    Run this command for each deployment.

1. To delete disk references, run:

    ```
    bosh cloud-check
    ```

    If the `bosh cloud-check` command does not successfully delete disk references, and you see a message similar to the example below, follow the additional procedures in [Remove Unused Disks](#removing-disks).
    <pre class="terminal">
    Scanning 19 persistent disks: 19 OK, 0 missing ...
    </pre>

### <a id='redeploy-ert'></a> Step 12: Redeploy <%= vars.product_short %>

These sections describe the steps to redeploy <%= vars.product_short %>.

#### <a id='determine-req-stemcells'></a> Determine the Required Stemcell

To determine which stemcell is used by <%= vars.product_short %>, either:

* Review the stemcell library:
    1. Go to **Stemcell Library**.
    1. Record the <%= vars.product_short %> stemcell release number from the **Staged** column.

* Review a stemcell list using BOSH CLI:
    1. To retrieve the stemcell release using the BOSH CLI, run:

        ```
        bosh -e BOSH-DIRECTOR-IP deployments
        ```
        Where `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
        <br><br>
        For example:
        <pre class="terminal">
        $ bosh -e BOSH-DIRECTOR-IP deployments
        Using environment '10.0.0.5' as user 'director' (bosh.\*.read, openid, bosh.\*.admin, bosh.read, bosh.admin)

        Name                     Release(s)                            Stemcell(s)                                    Team(s)  Cloud Config
        cf-9cb6995b7d746cd77438  push-apps-manager-release/661.1.24    bosh-google-kvm-ubuntu-trusty-go_agent/3421.9  -        latest
        ...
        </pre>

For more information about stemcells in <%= vars.ops_manager %>, see [Importing and Managing Stemcells](../../opsguide/managing-stemcells.html).

#### <a id='upload-stemcell'></a> Upload the Stemcell

To upload the required stemcell:

1. Download the stemcell from the [Pivotal Stemcells](https://network.pivotal.io/products/stemcells-ubuntu-xenial/) page on Pivotal Network.

1. Upload the stemcell used by <%= vars.product_short %> by running:

    ```
    bosh -e BOSH-DIRECTOR-IP \
    -d DEPLOYMENT-NAME \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    <%= vars.bosh_upload_stemcell %> \
    --fix PATH-TO-STEMCELL
    ```
    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `DEPLOYMENT-NAME` is the deployment name retrieved in [Step 10: Identify Your Deployment](#identify-deployment).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.
    * `PATH-TO-STEMCELL` is the path to your tile's stemcell.

1. To ensure the stemcells for all of your other installed tiles have been uploaded, repeat the last step, running `bosh <%= vars.bosh_upload_stemcell %> --fix PATH-TO-STEMCELL`, for each stemcell that is different from the already uploaded PAS stemcell.

#### <a id='redeploy-pas'></a> Redeploy <%= vars.product_short %>

To redeploy <%= vars.product_short %>:

1. Navigate to the <%= vars.ops_manager %> Installation Dashboard.

1. Click the <%= vars.product_short %> tile.

1. Select **Resource Config**.

1. Ensure the number of instances for MySQL Server is set to `1`.
  <p class="note warning"><strong>Warning:</strong> Restore fails if there is not exactly one MySQL Server instance deployed.</p>

1. Ensure that all errands needed by your system are set to **Run**.

1. Return to the <%= vars.ops_manager %> Installation Dashboard.

1. Click **Review Pending Changes**.

1. Review your changes and ensure the <%= vars.product_short %> tile is selected. Other tiles are optional. For more information, see [Reviewing Pending Product Changes](../review-pending-changes.html).

1. Click **Apply Changes** to redeploy.

### <a id='restore-services'></a> Step 13: (Optional) Restore Service Data

<p class="note warning"><strong>WARNING:</strong> BBR does not back up or restore any service data.</p>

For this step, you must restore data to pre-provisioned service tiles.

The procedures for restoring service data vary. For more information, see the documentation for your service tile.

For example, if you are using Redis for <%= vars.product_name %> v1.14, see [Using BOSH Backup and Restore (BBR)](https://docs.pivotal.io/redis/1-14/bbr-backup.html).

### <a id='restore-ert'></a> Step 14: Restore <%= vars.product_short %>

<div class="note"><strong>Notes:</strong>
  <ul>
    <li>The BBR <%= vars.product_short %> restore command can take at least 15 minutes to complete.</li>
    <li><%= vars.recommended_by %> recommends that you run it independently of the SSH session, so that the process can continue running even if your connection to the jumpbox fails. The command above uses <code>nohup</code>, but you could also run the command in a <code>screen</code> or <code>tmux</code> session.</li>
  </ul>
</div>

To restore your <%= vars.product_short %> backup:

1. See the table in [External Blobstore Support](enabling-external-blobstore-backups.html#blobstore-support) in _Enabling External Blobstore Backups_. If external blobstore support is not included in the version of <%= vars.ops_manager %> and <%= vars.product_short %> you are using, restore the external blobstore with your IaaS-specific tools before restoring <%= vars.product_short %>.

1. From your jumpbox, run:

    ```
    bbr deployment \
    --target BOSH-DIRECTOR-IP \
    --username BOSH-CLIENT \
    --password BOSH-PASSWORD \
    --deployment DEPLOYMENT-NAME \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    restore \
    --artifact-path PATH-TO-<%= vars.product_short %>-BACKUP
    ```
    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `BOSH-CLIENT` and `BOSH-PASSWORD` are the **Uaa Bbr Client Credentials**, identity and password that you retrieved in [Step 7: Retrieve BOSH Director Credentials](#retrieve-credentials).
    * `DEPLOYMENT-NAME` is the deployment name retrieved in [Step 10: Identify Your Deployment](#identify-deployment).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.
    * `PATH-TO-<%= vars.product_short %>-BACKUP` is the path to the <%= vars.product_short %> backup you want to restore.

1. If desired, scale the MySQL Server job back up to its previous number of instances in the **Resource Config** section of the <%= vars.product_short %> tile. After scaling the job, return to the <%= vars.ops_manager %> Installation Dashboard.

1. Click **Review Pending Changes**.

1. Review your changes. For more information, see [Reviewing Pending Product Changes](../review-pending-changes.html).

1. Click **Apply Changes** to deploy.

### <a id='odb'></a> Step 15: (Optional) Restore On-Demand Service Instances

If you have on-demand service instances provisioned by an on-demand service broker, you can restore them after successfully restoring <%= vars.product_short %>.

To restore your on-demand service instances:

1. Navigate to an on-demand service tile in the <%= vars.ops_manager %> Installation Dashboard.

1. Select the **Errands** tab.

1. Ensure the **Upgrade All Service Instances** errand is **On**.

1. Repeat for all on-demand service tiles.

1. Return to the <%= vars.ops_manager %> Installation Dashboard.

1. Click **Review Pending Changes**, review your changes, and then click **Apply Changes**. This includes running the **Upgrade All Service Instances** errand for the on-demand service, which redeploys the on-demand service instances.

1. (Optional) Restore service data to every on-demand service instance.

1. Any app on <%= vars.product_short %> bound to an on-demand service instance might need to be restarted to start consuming the restored on-demand service instances.

### <a id='non-tiles'></a> Step 16: (Optional) Restore Non-Tile Deployments

If you have any deployments that were deployed manually with the BOSH Director rather than through an <%= vars.ops_manager %> tile, you must restore their VMs.

To restore the VMs:

1. To obtain a list of all deployments on your BOSH Director, run:

    ```
    bosh -e BOSH-DIRECTOR-IP \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    deployments
    ```
    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.

1. Identify the names of the deployments that you need to restore. Do not include the deployments from <%= vars.ops_manager %> tiles.

1. For each deployment you need to restore, run:

    ```
    bosh -n -e BOSH-DIRECTOR-IP \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    -d DEPLOYMENT-NAME \
    cck --resolution=recreate_vm
    ```
    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.
    * `DEPLOYMENT-NAME` is the deployment name retrieved in [Step 10: Identify Your Deployment](#identify-deployment).

1. To verify the status of the VMs in each deployment, run:

    ```
    bosh -e BOSH-DIRECTOR-IP \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    -d DEPLOYMENT-NAME \
    vms
    ```
    Where:
    <br>
    * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.
    * `DEPLOYMENT-NAME` is the deployment name retrieved in [Step 10: Identify Your Deployment](#identify-deployment).

The process state for all VMs should show as `running`.


## <a id="after-restore"></a> After Restoring Your Backup

These sections describe the procedures you must perform after restoring your <%= vars.product_name %> backup with BBR.

### <a id='removing-disks'></a> Step 17: Remove Unused Disks

<p class="note warning"><strong>Warning:</strong> This is a very destructive operation.</p>

Disks from a previous deployment prevent recreated deployments from working.

#### <a id='bosh-clean-up-disks'></a> Use BOSH to Clean Up Disks

To clean up disk references:

1. Run:

    ```
    bosh cloud-check
    ```

#### <a id='manually-clean-up-disks'></a> Manually Clean Up Disks

If `bosh cloud-check` does not clean up all disk references, you must manually delete the remaining disks.

To delete the remaining disks, follow one of these procedures:

  * Use the BOSH CLI to delete the disks:
    1. Target the redeployed BOSH Director using the BOSH CLI by following the procedures in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
    1. List the deployments by running:

          ```
          bosh -e BOSH-DIRECTOR-IP \
          --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE deployments
          ```
          Where:
          <br>
          * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
          * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.
    1. Delete each deployment by running:

          ```
          bosh -d DEPLOYMENT-NAME <%= vars.bosh_delete_deployment %>
          ```
          Where:
          <br>
          * `DEPLOYMENT-NAME` is the deployment name retrieved in [Step 10: Identify Your Deployment](#identify-deployment).
  * Log in to your IaaS account and delete the disks manually.
      1. To retrieve a list of disk IDs, run:

          ```
          bosh -e BOSH-DIRECTOR-IP \
          --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE instances -i
          ```
          Where:
          <br>
          * `BOSH-DIRECTOR-IP` is the BOSH Director IP retrieved in [Step 8: Retrieve BOSH Director Address](#retrieve-address).
          * `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the CA certificate for the BOSH Director, if the certificate is not verifiable by the local machine's certificate chain.

After the disks are deleted, continue with [Step 11: Remove Stale Cloud IDs for All Deployments](#bosh-cck).
