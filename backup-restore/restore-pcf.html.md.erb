---
title: Restoring Pivotal Cloud Foundry Manually from Backup
owner: RelEng
---

<strong><%= modified_date %></strong>

<style>
    .note.warning {
       background-color: #fdd;
       border-color: #fbb
  	          }
    .note.warning:before {
       color: #f99;
		         }
</style>

<p class="note"><strong>Important</strong>: If you used BOSH Backup and Restore (BBR) to back up Pivotal Cloud Foundry (PCF), you must use BBR to restore. See the restore instructions <a href="./restore-pcf-bbr.html">here</a>.</p>

This topic describes the procedure for manually restoring your Pivotal Cloud Foundry (PCF) deployment from a backup. To create a backup, see the [Backing Up Pivotal Cloud Foundry](./backup-pcf.html) topic.

To restore a deployment, you must import installation settings, temporarily stop the Cloud Controller, restore the state of each critical backend component from its backup file, and restart the Cloud Controller. 

To perform these steps, you need the BOSH manifest to locate your critical backend components. BOSH manifests are automatically downloaded to the Ops Manager virtual machine. 

However, if you are using a separate jumpbox, you must manually download the BOSH deployment manifest.

<p class='note'><strong>Note</strong>:
The procedure described in this topic restores a running PCF deployment to the state captured by backup files. This procedure does not deploy PCF. See the <a href='../../installing/index.html'>Installing PCF Guide</a> for information.</p>

## <a id='import'></a>Step 1: Import Installation Settings ##

<p class='note'><strong>Note</strong>: Pivotal recommends that you export your installation settings before importing from a backup. See the <a href='./backup-pcf.html#export'>Export Installation Settings</a> section of the <i>Backing Up Pivotal Cloud Foundry</i> topic for more information.</p>

**Import installation settings** imports the settings and assets of an existing
PCF installation. Importing an installation overwrites any existing installation. You must provision a new Ops Manager to import settings.

Follow the steps below to import installation settings.

1. Select and follow the instruction below for your IaaS to deploy the new Ops Manager VM.
  * [Manually Configuring AWS for PCF](../pcf-aws-manual-om-config.html)
  * [Launching an Ops Manager Director Instance on AWS (for CloudFormation users)](../cloudform-om-deploy.html)
  * [Launching an Ops Manager Director Instance on Azure](../azure-arm-template.html)
  *  [Launching an Ops Manager Director Instance on GCP](../gcp-om-deploy.html)
  * [Provisioning the OpenStack Infrastructure](../openstack-setup.html)
  * [Deploying Operations Manager to vSphere](../deploying-vm.html)

1. When redirected to the **Welcome to Ops Manager** page, select **Import Existing Installation**.

    <%= image_tag("../images/upgrading/welcome.png") %>

1. In the import panel, perform the following tasks:
  * Enter your **Decryption Passphrase**.
  * Click **Choose File** and browse to the installation zip file that you exported in the [Export Installation Settings](./backup-pcf.html#export) section of this topic.

    <%= image_tag("../images/upgrading/decryption_passphrase.png") %>

1. Click **Import**.

    <p class="note"><strong>Note</strong>:
      Some browsers do not provide feedback on the status of the import process,
      and may appear to hang.</p>

1. A "Successfully imported installation" message appears upon completion.

    <%= image_tag("../images/upgrading/success.png") %>

1. Click **Apply Changes** to redeploy.

## <a id='restore_bosh'></a>Step 2: Restore BOSH Using Ops Manager ##

Follow the steps below to restore BOSH using Ops Manager.

1. From the **Product Installation Dashboard**, click the **Ops Manager Director** tile.

1. Make a change to your configuration to trigger a new deployment. For example, you can adjust the number of NTP servers in your deployment. Choose a change in configuration that suits your specific deployment.

1. SSH into the Ops Manager VM by following the instructions for your IaaS in the [SSH into Ops Manager](../../customizing/trouble-advanced.html#ssh) topic. 

1. Delete the `bosh-deployments.yml` file. Deleting the `bosh-deployments.yml` file causes Ops Manager to treat the deploy as a new deployment, recreating missing Virtual Machines (VMs), including BOSH. The new deployment ignores existing VMs such as your Pivotal Cloud Foundry deployment.
  <pre class='terminal'>
  $ sudo rm /var/tempest/workspaces/default/deployments/bosh-deployments.yml
  </pre>

1. Rename, move, or delete the `bosh-state.json` file. Removing this file causes Ops Manager to treat the deploy as a new deployment, recreating missing VMs, including BOSH. The new deployment ignores existing VMs such as your Pivotal Cloud Foundry deployment.
  <pre class='terminal'>
  $ cd /var/tempest/workspaces/default/deployments/
  $ sudo mv bosh-state.json bosh-state.json.old
  </pre>

1. Return to the **Product Installation Dashboard** and click **Apply Changes**.

## <a id="download_bosh_manifest"></a>Step 3: Download the BOSH Manifest

To download the BOSH manifest for your deployment, use the BOSH CLI (either v1 or v2) or access the Ops Manager API.

### Using BOSH CLI v1

First, identify and target the BOSH Director by performing the following steps:

<%= partial 'bosh_target_director_manual' %>

Next, download the BOSH manifest for the product by performing the following steps:

<%= partial 'download_bosh_manifest' %>

### Using BOSH CLI v2

First, target the BOSH Director by performing the following steps:

<%= partial 'bosh_target_director_bbr' %>

Next, identify the deployment and download the BOSH manifest for the product by performing the following steps:

1. After logging in to your BOSH Director, run the following command to identify the name of 
the BOSH deployment that contains PCF:
    <pre class="terminal">
    $ bosh -e DIRECTOR\_IP \
    --ca-cert /var/tempest/workspaces/default/root\_ca\_certificate deployments

    Name                     Release(s)
    cf-example               push-apps-manager-release/661.1.24
                             cf-backup-and-restore/0.0.1
                             binary-buildpack/1.0.11
                             capi/1.28.0
                             cf-autoscaling/91
                             cf-mysql/35
                             ...
    </pre>
    In the above example, the name of the BOSH deployment that contains PCF is `cf-example`.

1. Run the following command to download the BOSH manifest and replace DEPLOYMENT-NAME with the deployment name you retrieved in the prevoius step:

    <pre class="terminal">
    $ bosh -e DIRECTOR_IP -d DEPLOYMENT-NAME manifest > /tmp/cf.yml
    </pre>

1. Place the .yml file in a secure location.

### Using the Ops Manager API

First, identify and target the BOSH Director by performing the following steps:

1. Install the [BOSH v2 CLI](https://bosh.io/docs/cli-v2.html#install) on a machine outside of your PCF deployment.
1. Perform the procedures in the [Using the Ops Manager API](../ops-man-api.html) topic to authenticate and access the Ops Manager API.
1. Use the `GET /api/v0/deployed/products` endpoint to retrieve a list of deployed products, replacing `UAA-ACCESS-TOKEN` with the access token recorded in the [Using the Ops Manager API](../ops-man-api.html) topic:
  <pre class="terminal">$ curl "http<span>s:</span>//OPS-MAN-FQDN/api/v0/deployed/products" \ 
    -X GET \ 
    -H "Authorization: Bearer UAA-ACCESS-TOKEN"</pre>
1. In the response to the above request, locate the product with an `installation_name` starting with `cf-` and copy its `guid`.
1. Run the following `curl` command, replacing `PRODUCT-GUID` with the value of `guid` from the previous step:
    <pre class="terminal">
    $ curl "http<span>s:</span>//OPS-MAN-FQDN/api/v0/deployed/products/PRODUCT-GUID/manifest" \ 
    -X GET \
    -H "Authorization: Bearer UAA-ACCESS-TOKEN" > /tmp/cf.yml</pre>

1. Place the .yml file in a secure location.

## <a id='stop-cc'></a>Step 4: Stop Cloud Controller###

To stop the Cloud Controller, use the BOSH CLI (either v1 or v2).

### Using BOSH CLI v1

<%= partial 'stopping_cc' %>

### Using BOSH CLI v2

<%= partial 'stopping_cc_boshv2' %>

## <a id='restore-databases'></a>Step 5: Restore Critical Backend Components##

Your Elastic Runtime deployment contains several critical data stores that must be present for a complete restore. This section describes the procedure for restoring the databases and servers associated with your PCF installation.

You must restore each of the following:

* Cloud Controller Database
* UAA Database
* WebDAV Server
* Pivotal MySQL Server


### <a id='restore-webdav'></a> Restore WebDAV

Use the File Storage password and IP address to restore the WebDAV server by following the steps detailed below. Find the IP address in your BOSH deployment manifest. To find your password in the Ops Manager **Installation Dashboard**, select **Elastic Runtime** and click **Credentials>Link to Credential**.

1. Run `ssh YOUR-WEBDAV-VM-IP-ADDRESS` to enter the WebDAV VM.
<pre class='terminal'>
$ ssh <span>vcap</span>@192.0.2.10
</pre>

1. Log in as root user. When prompted for a password, enter the vcap password you used to `ssh` into the VM:
<pre class='terminal'>$ sudo su</pre>

1. Temporarily change the permissions on `var/vcap/store` to add write permissions for all.
<pre class='terminal'>
$ chmod a+w /var/vcap/store
</pre>

1. Use `scp` to send the WebDAV backup tarball to the WebDAV VM from your local machine.
<pre class='terminal'>
$ scp webdav.tar.gz vcap@YOUR-WEBDAV-VM-IP-ADDRESS:/var/vcap/store
</pre>

1. Navigate into the `store` folder on the WebDAV VM.
<pre class='terminal'>
$ cd /var/vcap/store
</pre>

1. Decompress and extract the contents of the backup archive.
<pre class='terminal'>
$ tar zxf webdav.tar.gz</pre>
</pre>

1. Change the permissions on `var/vcap/store` to their prior setting.
<pre class='terminal'>
$ chmod a-w /var/vcap/store
</pre>

1. Exit the WebDAV VM.
<pre class='terminal'>
$ exit
</pre>

### <a id='restore-mysql-dev'></a>Restore MySQL Database ###

Restoring from a backup is the same whether one or multiple databases were backed up. Executing the SQL dump will drop, recreate, and refill the specified databases and tables.

<p class="note warning"><strong>Warning</strong>: Restoring a database deletes all data that existed in the database prior to the restore. Restoring a database using a full backup artifact, produced by <code>mysqldump --all-databases</code> for example, replaces all data and user permissions.</p>

There are two ways to restore the MySQL Server:

* [Automatic backup](#automatic-mysql): If you configured automatic backups in your ERT configuration, follow the instructions below for restoring from an automatic backup
* [Manual restore](#manual-mysql): If you performed a [manual backup](backup-pcf.html#manual-mysql) of your MySQL server, follow the instructions below for restoring from a manual backup.

#### <a id='automatic-mysql'></a>Restore MySQL from an Automatic Backup

If you configured automatic backups, perform the following steps to restore your MySQL server:

1. If you are running a highly available ERT MySQL cluster, perform the following steps to reduce the size of the cluster to a single node:
    1. From the Ops Manager Installation Dashboard, click the **Elastic Runtime** tile.
    1. Click **Resource Config**.
    1. Set the number of instances for **MySQL Server** to `1`.
    1. Click **Save**.
    1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.
1. After the deployment finishes, perform the following steps to prepare the first node for restoration:
    1. Retrieve the IP address for the MySQL server by navigating to the **Elastic Runtime** tile and clicking the **Status** tab.
    1. Retrieve the MySQL VM credentials for the MySQL server by navigating to the **Elastic Runtime** tile and clicking the **Credentials** tab.
    1. SSH into the Ops Manager Director. For more information, see the previous section [Restore BOSH Using Ops Manager](#restore_bosh).
    1. From the Ops Manager Director VM, use the BOSH CLI to SSH into the first MySQL job. For more information, see the [BOSH SSH](../../customizing/trouble-advanced.html#bosh-ssh) section in the <em>Advanced Troubleshooting with the BOSH CLI</em> topic.
    1. On the MySQL server VM, become super user:
      <pre class="terminal">$ sudo su</pre>
    1. Pause the local database server:
      <pre class="terminal">$ monit stop all</pre>
    1. Confirm that all jobs are listed as `not monitored`:
      <pre class="terminal">$ watch monit summary</pre>
    1. Delete the existing MySQL data that is stored on disk:
      <pre class="terminal">$ rm -rf /var/vcap/store/mysql/*</pre>
1. Perform the following steps to restore the backup:
    1. Move the compressed backup file to the node using `scp`.
    1. Decrypt and expand the file using `gpg`, sending the output to tar:
      <pre class="terminal">$ gpg --decrypt mysql-backup.tar.gpg | tar -C /var/vcap/store/mysql -xvf -</pre>
    1. Change the owner of the data directory, because MySQL expects the data directory to be owned by a particular user:
      <pre class="terminal">$ chown -R vcap:vcap /var/vcap/store/mysql</pre>
    1. Start all services with `monit`:
      <pre class="terminal">$ monit start all</pre>
    1. Watch the summary until all jobs are listed as `running`:
      <pre class="terminal">$ watch monit summary</pre>
    1. Exit out of the MySQL node.
1. If you are restoring a highly available ERT MySQL cluster, perform the following steps to increase the size of the cluster back to three:
    1. From the Ops Manager Installation Dashboard, click the **Elastic Runtime** tile.
    1. Click **Resource Config**.
    1. Set the number of instances for **MySQL Server** to `3`.
    1. Click **Save**.
    1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.

#### <a id='manual-mysql'></a>Restore MySQL from a Manual Backup

If you performed a manual backup, perform the following steps to restore your MySQL server:

1. If you are running a highly available ERT MySQL cluster, perform the following steps to reduce the size of the cluster to a single node:
    1. From the Ops Manager Installation Dashboard, click the **Elastic Runtime** tile.
    1. Click **Resource Config**.
    1. Set the number of instances for **MySQL Server** to `1`.
    1. Click **Save**.
    1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.
1. Retrieve the IP address of the MySQL server by navigating to the **Elastic Runtime** tile in the Ops Manager Installation Dashboard and clicking the **Status** tab.
1. Set the IP address of the MySQL server as an environment variable:
  <pre class="terminal">$ MYSQL\_NODE\_IP='YOUR-MYSQL-IP'</pre>
1. Retrieve the MySQL VM credentials and the MySQL Admin credentials by navigating to the **Elastic Runtime** tile and clicking the **Credentials** tab.
1. Locate the `ert_databases.sql` backup file that you created when performing a [manual backup](backup-pcf.html#manual-mysql).
1. Use `scp` to send the backup file to the MySQL Database VM: 
  <pre class='terminal'>
  $ scp ert_databases.sql vcap@$MYSQL\_NODE\_IP:~/.
  </pre>
1. SSH into the MySQL Database VM, providing the MySQL VM password when prompted:
  <pre class='terminal'>
  $ ssh vcap@$MYSQL\_NODE\_IP
  </pre>
1. Enable the creation of tables using any storage engine, providing the MySQL Admin password when prompted: 
  <pre class='terminal'>
  $ mysql -h $MYSQL\_NODE\_IP -u root -p -e "SET GLOBAL enforce\_storage\_engine=NULL;"
  </pre>
1. Use the MySQL password and IP address to restore the MySQL database by running the following command.
  <pre class='terminal'>
  $ mysql -h $MYSQL\_NODE\_IP -u root -p < ~/.ert_databases.sql
  </pre>
1. Use the MySQL password and IP address to restore original storage engine restriction.
  <pre class='terminal'>
  $ mysql -h $MYSQL\_NODE\_IP -u root -p -e "SET GLOBAL enforce\_storage\_engine='InnoDB';"
  </pre>
1. Log in to the MySQL client and flush privileges.
  <pre class='terminal'>
  $ mysql -u root -p -h
  mysql > flush privileges;
  </pre>
1. If you are restoring a highly available ERT MySQL cluster, perform the following steps to increase the size of the cluster back to three:
    1. From the Ops Manager Installation Dashboard, click the **Elastic Runtime** tile.
    1. Click **Resource Config**.
    1. Set the number of instances for **MySQL Server** to `3`.
    1. Click **Save**.
    1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.

## <a id='cc-start-2'></a>Step 6: Start Cloud Controller ###

To start the Cloud Controller, use the BOSH CLI (either v1 or v2).

### Using BOSH CLI v1

<%= partial 'starting_cc' %>

### Using BOSH CLI v2

<%= partial 'starting_cc_boshv2' %>
