---
title: Restoring Pivotal Cloud Foundry from Backup with BBR
owner: RelEng
---

This topic describes the procedure for restoring your critical backend Pivotal Cloud Foundry (PCF) components with BOSH Backup and Restore (BBR), a command-line tool for backing up and restoring BOSH deployments. To perform the procedures in this topic, you must have backed up PCF by following the steps in the [Backing Up Pivotal Cloud Foundry with BBR](backup-pcf-bbr.html) topic.

To view the BBR release notes, see the <a href="https://docs.pivotal.io/bbr/bbr-rn.html">BOSH Backup and Restore Release Notes</a>. To restore PCF manually, see the <a href="restore-pcf.html">Restoring Pivotal Cloud Foundry Manually from Backup</a> topic.

The procedures described in this topic prepare your environment for PCF, deploy Ops Manager, import your installation settings, and use BBR to restore your PCF components.

<p class="note warning"><strong>Warning</strong>: Restoring Pivotal Cloud Foundry (PCF) with BBR is a destructive operation. If the restore fails, the new environment may be left in an unusable state and require reprovisioning. Only perform the procedures in this topic for the purpose of disaster recovery, such as recreating PCF after a storage-area network (SAN) corruption.</p>

<p class="note warning"><strong>Warning</strong>: When validating your backup, the VMs and disks from the backed-up BOSH Director should not visible to the new BOSH Director. As a result, Pivotal recommends that you deploy the new BOSH Director to a different IaaS network and account than the VMs and disks of the backed up BOSH Director.</p>

<p class="note"><strong>Note</strong>: BBR is a feature in PCF v1.11. You can only use BBR to back up PCF v1.11 and later. To restore earlier versions of PCF, perform the <a href="restore-pcf.html">manual procedures</a>.</p>

<p class="note"><strong>Note</strong>: If the BOSH Director you are restoring had any deployments that were deployed manually rather than through an Ops Manager Tile, you must restore them manually at the end of the process. For more information, see <a href="#non-tiles">(Optional) Step 14: Restore Non-Tile Deployments</a>.</p>

<p class="note validation"><strong>Note</strong>: If you are restoring in order to validate a backup, look for notes marked <strong>Validation</strong> throughout the topic.</p>

## <a id="compatibility"></a> Compatibility of Restore

This section describes the restrictions for a backup artifact to be restorable to another environment.
This section is for guidance only, and Pivotal highly recommends that operators validate their backups by
using the backup artifacts in a restore.

Consult the following restrictions for a backup artifact to be restorable:

+ **CIDR ranges**: BBR requires the IP address ranges to be the same in the restore environment as in the backup environment.
+ **Topology**: BBR requires the BOSH topology of a deployment to be the same in the restore environment as it was in the backup environment.
+ **Naming of instance groups and jobs**: For any deployment that implements the backup and restore scripts, the instance groups and jobs must have the same names.
+ **Number of instance groups and jobs**: For instance groups and jobs that have backup and restore scripts, there must be the same number of instances.
+ **Limited validation**: BBR puts the backed up data into the corresponding instance groups and jobs in the restored environment,
    but can’t validate the restore beyond that.
    For example, if the MySQL encryption key is different in the restore environment,
    the BBR restore might succeed although the restored MySQL database is unusable.
+ **PCF version**: BBR can restore to the same version of PCF that was backed up. BBR does not support restoring to other major, minor, or patch releases.

<p class="note"><strong>Note</strong>: A change in VM size or underlying hardware should not affect BBR’s ability to restore data, as long as there is adequate storage space to restore the data. </p>

## <a id='prepare-env'></a>(Optional) Step 1: Prepare Your Environment

In an event of a disaster, you may lose not only your VMs and disks, but your IaaS resources as well, such as networks and load balancers.

If you need to recreate your IaaS resources, prepare your environment for PCF by following the instructions specific to your IaaS in <a href='../../installing/index.html'>Installing Pivotal Cloud Foundry</a>.

<p class="note"><strong>Note</strong>: The instructions for installing PCF on Amazon Web Services (AWS) and OpenStack combine the procedures for preparing your environment and deploying Ops Manager into a single topic. The instructions for the other supported IaaSes split these procedures into two separate topics.</p>

If you recreate your IaaS resources, you must also add those resources to Ops Manager by performing the procedures in the [(Optional) Step 3: Configure Ops Manager for New Resources](#config-new-resources) section.

## <a id='deploy-import'></a>Step 2: Deploy Ops Manager and Import Installation Settings ##

1. Perform the procedures for your IaaS to deploy Ops Manager:
  * If you configured AWS manually: [Step 12](../pcf-aws-manual-config.html#pcfaws-om-ami) through [Step 19](../pcf-aws-manual-config.html#pcfaws-mysql-rds) of the <em>Manually Configuring AWS for PCF</em> topic.
  * If you used the CloudFormation template install PCF on AWS: [Launching an Ops Manager Director Instance on AWS](../cloudform-om-deploy.html)
  * [Launching an Ops Manager Director Instance on Azure](../azure-arm-template.html)
  * [Launching an Ops Manager Director Instance on GCP](../gcp-om-deploy.html)
  * [Step 3](../openstack-setup.html#create-om-image) through [Step 7](../openstack-setup.html#dns-entry) of the <em>Provisioning the OpenStack Infrastructure</em> topic.
  * [Deploying Operations Manager to vSphere](../deploying-vm.html)

1. Access your new Ops Manager by navigating to `YOUR-OPS-MAN-FQDN` in a browser.

1. On the **Welcome to Ops Manager** page, click **Import Existing Installation**.

    <%= image_tag("../images/upgrading/welcome.png") %>

1. In the import panel, perform the following tasks:
  * Enter your **Decryption Passphrase**.
  * Click **Choose File** and browse to the installation zip file that you exported in the <a href='./backup-pcf-bbr.html#export'>Step 7: Export Installation Settings</a> section of the <i>Backing Up Pivotal Cloud Foundry with BBR</i> topic.

    <%= image_tag("../images/upgrading/decryption_passphrase.png") %>

1. Click **Import**.

    <p class="note"><strong>Note</strong>:
      Some browsers do not provide feedback on the status of the import process,
      and may appear to hang. The import process takes at least 10 minutes, and takes longer the more tiles that were present on the backed-up Ops Manager.</p>

1. A **Successfully imported installation** message appears upon completion.

    <%= image_tag("../images/upgrading/success.png") %>

## <a id="config-new-resources"></a> (Optional) Step 3: Configure Ops Manager for New Resources

If you recreated IaaS resources such as networks and load balancers by following the steps in the [(Optional) Step 1: Prepare Your Environment](#prepare-env) section above, perform the following steps to update Ops Manager with your new resources:

1. Enable the Ops Manager advanced mode following this [Knowledge Base article](https://discuss.pivotal.io/hc/en-us/articles/219118768-How-to-enable-advanced-mode-in-the-Ops-Manager).
1. Navigate to the Ops Manager Installation Dashboard and click the Ops Manager Director tile.
1. Click **Create Networks** and update the network names to reflect the network names for the new environment.
1. If running on GCP, click **Google Config** and update the **Project ID** to reflect the new GCP project ID.
1. Return to the Ops Manager Installation Dashboard and click the Elastic Runtime tile.
1. Click **Resource Config**. If necessary for your IaaS, enter the name of your new load balancer in the **Load Balancer** column.
1. If necessary, click **Networking** and update the load balancer SSL certificate and private key under **Router SSL Termination Certificate and Private Key**.
1. If your environment has a new DNS address, update the old environment DNS entries to point to the new load balancer addresses. For more information, see the [Step 4: Configure Networking](../custom-load-balancer.html#configure-networking) section of the <em>Using Your Own Load Balancer</em> topic and follow the link to the instructions for your IaaS.
1. If you are using Google Cloud Platform (GCP), navigate to the **Google Config** section of the Ops Manager Director tile and update the **Default Deployment Tag** to reflect the new environment.
1. Make sure you disable the Ops Manager advanced mode, as recommended in the [Knowledge Base article](https://discuss.pivotal.io/hc/en-us/articles/219118768-How-to-enable-advanced-mode-in-the-Ops-Manager).

## <a id="bosh-state"></a> Step 4: Remove BOSH State File

1. SSH into your Ops Manager VM. For more information, see the [SSH into Ops Manager](../trouble-advanced.html#ssh) section of the <em>Advanced Troubleshooting with the BOSH CLI</em> topic.
1. On the Ops Manager VM, delete the `/var/tempest/workspaces/default/deployments/bosh-state.json` file:
  <pre class="terminal">
  $ rm /var/tempest/workspaces/default/deployments/bosh-state.json
  </pre>
1. Navigate to `YOUR-OPS-MAN-FQDN` in a browser and log into Ops Manager.
1. Upload the required stemcell for each tile that requires one. Each tile that requires an upload displays with an orange indicator at the bottom. When clicked, the **Stemcell** panel shows an orange circle beside it.
<p class="note warning"><strong>Warning</strong>: Do not click <strong>Apply Changes</strong> at this point.</p>

## <a id="bosh-only-deploy"></a> Step 5: Deploy Ops Manager Director

Perform the steps in the [Applying Changes to Ops Manager Director](../deploy-ops-man-director.html) topic to use the Ops Manager API to only deploy the Ops Manager Director.
<p class="note"><strong>Validation</strong>: If your BOSH Director has an external hostname, you should change it in **Ops Manager Director > Director Config > Director Hostname** to ensure it does not conflict with the hostname of the backed up Director.</p>

## <a id="artifacts-jumpbox"></a> Step 6: Transfer Artifacts to Jumpbox

In the [Step 9: Back Up Your Elastic Runtime Deployment](./backup-pcf-bbr.html#bbr-backup) section of the <em>Backing Up Pivotal Cloud Foundry with BBR</em> topic, in the <em>After Taking the Backups</em> section you moved the TAR and metadata files of the backup artifacts off your jumpbox to your preferred storage space. Now you must transfer those files back to your jumpbox.

For instance, you could SCP the backup artifact to your jumpbox:
  <pre class="terminal">$ scp LOCAL-PATH-TO-BACKUP-ARTIFACT JUMPBOX-USER/JUMPBOX-ADDRESS</pre>

<p class="note"><strong>Note</strong>: Pivotal recommends that you regularly update the BBR binary on your jumpbox to the latest version</a>. See <a href="system-setup-bbr.html#scp">Transfer BBR Binary to Your Jumpbox</a> in <em>Setting Up Your Jumpbox for BBR</em> for more information.</p>

## <a id="retrieve"></a> Step 7: Retrieve BOSH Director Address and Credentials

Perform the following steps to retrieve the IP address of your BOSH Director and the credentials for logging in from the Ops Manager Director tile:

<%= partial 'bosh_target_director_bbr' %>


## <a id='restore-bosh'></a> Step 8: Restore the BOSH Director

1. Navigate to the Ops Manager Installation Dashboard.
1. Click the Ops Manager tile.
1. Click the **Credentials** tab.
1. Locate **Bbr Ssh Credentials** and click **Link to Credential** next to it.
<br><br>
    You can also retrieve the credentials using the Ops Manager API with a GET request to the following endpoint: `/api/v0/deployed/director/credentials/bbr_ssh_credentials`. For more information, see the [Using the Ops Manager API](../ops-man-api.html) topic.

1. Copy the value for `private_key_pem`.
1. SSH into your jumpbox.
1. Run the following command to reformat the key and save it to a file named `PRIVATE-KEY` in the current directory, pasting in the contents of your private key for `YOUR-PRIVATE-KEY`:
  <pre class="terminal">
  $ printf -- "YOUR-PRIVATE-KEY" > PRIVATE-KEY
  </pre>
1. Ensure the BOSH Director backup artifact is in the folder from which you run BBR.
1. Run the BBR restore command from your jumpbox to restore the BOSH Director:
  <pre class="terminal">
  $ nohup bbr director \
      --private-key-path PRIVATE-KEY \
      --username bbr \
      --host HOST \
      restore \
        --artifact-path PATH-TO-DIRECTOR-BACKUP
  </pre>
  Use the optional `--debug` flag to enable debug logs. See the [Logging](backup-pcf-bbr.html#logging) section of the <em>Backing Up Pivotal Cloud Foundry with BBR</em> topic for more information.
  <br><br>
  Replace the placeholder values as follows:
  * `PATH-TO-DIRECTOR-BACKUP`: This is the path to the Director backup you want to restore.
  * `PRIVATE-KEY`: This is the path to the private key file you created above.
  * `HOST`: This is the address of the BOSH Director. If the BOSH Director is public, this is a URL, such as `https://my-bosh.xxx.cf-app.com`. Otherwise, it is the `BOSH-DIRECTOR-IP`, which you retrieved in the [Step 6: Retrieve BOSH Director Address and Credentials](#retrieve) section.

<p class="note"><strong>Note</strong>: The BBR BOSH Director restore command can take at least 15 minutes to complete. Pivotal recommends that you run it independently of the SSH session, so that the process can continue running even if your connection to the jumpbox fails. The command above uses <code>nohup</code> but you could also run the command in a <code>screen</code> or <code>tmux</code> session.</p>

If the commands completes successfully, continue to [Step 9: Identify Your Deployment](#identify-deployment).

If the command fails, do the following:

1. Run restore-cleanup
<pre class="terminal">
$ bbr director \
  --private-key-path PRIVATE-KEY \
  --username bbr \
  --host HOST \
  backup-cleanup
</pre>

1. Ensure all the parameters in the command are set.
1. Ensure the BOSH Director credentials are valid.
1. Ensure the specified deployment exists.
1. Ensure the source deployment is compatible with the target deployment.
1. Ensure that the jumpbox can reach the BOSH Director.

## <a id='identify-deployment'></a> Step 9: Identify Your Deployment

After logging in to your BOSH Director, run `bosh deployments` to identify the name of the BOSH deployment that contains PCF:

<pre class='terminal'>
$ bosh -e DIRECTOR-IP --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE deployments

Name                     Release(s)
cf-example               push-apps-manager-release/661.1.24
                         cf-backup-and-restore/0.0.1
                         binary-buildpack/1.0.11
                         capi/1.28.0
                         cf-autoscaling/91
                         cf-mysql/35
                         ...
</pre>

In the above example, the name of the BOSH deployment that contains PCF is `cf-example`. `PATH-TO-BOSH-SERVER-CERTIFICATE` is the path to the Certificate Authority (CA) certificate for the BOSH Director. For more information, see [Ensure BOSH Director Certificate Availability](./system-setup-bbr.html#root-ca-cert).

## <a id="bosh-cck"></a> Step 10: Remove Stale Cloud IDs for All Deployments

For every deployment in the BOSH Director, run the following command:

<pre class="terminal">
$ bosh -e DIRECTOR-IP -d DEPLOYMENT-NAME -n cck \
  --resolution delete_disk_reference \
  --resolution delete_vm_reference
</pre>

This reconciles the BOSH Director's internal state with the state in the IaaS. You can use the list of deployments returned in [Step 9: Identify Your Deployment](#identify-deployment).

If the <code>bosh cck</code> command does not successfully delete disk references and you see a message similar to the following, perform the additional procedures in the <a href="#removing-disks">Remove Unused Disks</a> section below.
<pre class="terminal">
Scanning 19 persistent disks: 19 OK, 0 missing ...
</pre>

## <a id='redeploy-ert'></a> Step 11: Redeploy Elastic Runtime

1. Perform the following steps to determine which stemcell is used by Elastic Runtime:
  1. Navigate to the Ops Manager Installation Dashboard.
  1. Click the **Pivotal Elastic Runtime** tile.
  1. Click **Stemcell** and record the release number included in the displayed filename:
      <%= image_tag('images/stemcell.png') %>
    In the screenshot above, the stemcell release number is **3421.9**.
    <br><br>
    You can also retrieve the stemcell release using the BOSH CLI:
      <pre class="terminal">
        $ bosh -e DIRECTOR-IP deployments
        Using environment '10.0.0.5' as user 'director' (bosh.\*.read, openid, bosh.\*.admin, bosh.read, bosh.admin)

        Name                     Release(s)                            Stemcell(s)                                    Team(s)  Cloud Config
        cf-9cb6995b7d746cd77438  push-apps-manager-release/661.1.24    bosh-google-kvm-ubuntu-trusty-go_agent/3421.9  -        latest
        ...
      </pre>

1. Download the stemcell from [Pivotal Network](https://network.pivotal.io/products/stemcells).

1. Run the following command to upload the stemcell used by Elastic Runtime:
  <pre class="terminal">
  $ bosh -e BOSH-DIRECTOR-IP \
      -d DEPLOYMENT-NAME \
      --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
      upload-stemcell \
      --fix PATH-TO-STEMCELL
  </pre>
1. If you have any other tiles installed, ensure you upload their stemcells if they are different from the Elastic Runtime stemcell. Upload stemcells to the BOSH Director with `bosh upload-stemcell --fix PATH-TO-STEMCELL`, as in the command above.
1. From the Ops Manager Installation Dashboard, navigate to **Pivotal Elastic Runtime > Resource Config**.
1. Ensure the number of instances for MySQL Server is set to `1`.
  <p class="note warning"><strong>Warning</strong>: Restore fails if there is not exactly one MySQL Server instance deployed.</p>
1. Ensure that all errands needed by your system are set to run.
1. Return to the Ops Manager Installation Dashboard and click **Apply Changes** to redeploy.
  <p class="note validation"><strong>Validation</strong>: If your Elastic Runtime uses an external blobstore, ensure that the Elastic Runtime tile is configured to use a different blobstore before clicking <strong>Apply Changes</strong>. Otherwise it attempts to connect to the blobstore that the existing Elastic Runtime uses.</p>
  <p class="note validation"><strong>Validation</strong>: Ensure your **System Domain** and **Apps Domain** under **Pivotal Elastic Runtime > Domains** are updated to refer to the validation environment.
  </p>

## <a id='restore-ert'></a> Step 12: Restore Elastic Runtime

<p class="note validation"><strong>Validation</strong>: If your apps must not be running after a restore, run <code>bosh stop</code> on each <code>diego_cell</code> VM in the deployment.</p>

1. Run the BBR restore command from your jumpbox to restore Elastic Runtime:
<pre class="terminal">
$ BOSH_CLIENT_SECRET=BOSH-PASSWORD \
      bbr deployment \
        --target BOSH-DIRECTOR-IP \
        --username BOSH-CLIENT \
        --deployment DEPLOYMENT-NAME \
        --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
        restore \
          --artifact-path PATH-TO-ERT-BACKUP
</pre>

    Replace the placeholder values as follows:
    <br><br>
    * `BOSH-CLIENT`, `BOSH-PASSWORD`: Use the BOSH UAA user provided in  **Pivotal Ops Manager > Credentials > Uaa Bbr Client Credentials**.
    <br><br>
       You can also retrieve the credentials using the Ops Manager API with a GET request to the following endpoint: `/api/v0/deployed/director/credentials/uaa_bbr_client_credentials`. For more information, see the [Using the Ops Manager API](../ops-man-api.html) topic.<br><br>
    * `BOSH-DIRECTOR-IP`: You retrieved this value in the [Step 6: Retrieve BOSH Director Address and Credentials](#retrieve) section.
    * `DEPLOYMENT-NAME`: You retrieved this value in the [Step 8: Identify Your Deployment](#identify-deployment) section.
    * `PATH-TO-BOSH-SERVER-CERTIFICATE`: This is the path to the BOSH Director’s Certificate Authority (CA) certificate, if the certificate is not verifiable by the local machine's certificate chain.
    * `PATH-TO-ERT-BACKUP`: This is the path to the Elastic Runtime backup you want to restore.

    <p class="note validation">
      <strong>Validation</strong>: If you ran <code>bosh stop</code> on each <code>diego_cell</code> before running <code>bbr restore</code>, you can now run <code>cf stop</code> on all apps and then run <code>bosh start</code> on each <code>diego_cell</code>. After this, all apps are deployed in a stopped state.
    </p>

1. Perform the following steps after restoring Elastic Runtime:
  1. Retrieve the MySQL admin password by following one of the procedures below:
        * Log in to Ops Manager and navigate to **Pivotal Elastic Runtime > Credentials > Mysql Admin Credentials**.
        * Retrieve the credentials using the Ops Manager API by performing the following steps:
          1. Perform the procedures in the [Using the Ops Manager API](../ops-man-api.html) topic to authenticate and access the Ops Manager API.
          1. Use the `GET /api/v0/deployed/products` endpoint to retrieve a list of deployed products, replacing `UAA-ACCESS-TOKEN` with the access token recorded in the [Using the Ops Manager API](../ops-man-api.html) topic:
              <pre class="terminal">$ curl "http<span>s:</span>//OPS-MAN-FQDN/api/v0/deployed/products" \
                -X GET \
                -H "Authorization: Bearer UAA-ACCESS-TOKEN"</pre>
          1. In the response to the above request, locate the product with an `installation_name` starting with `cf-` and copy its `guid`.
          1. Run the following `curl` command, replacing `PRODUCT-GUID` with the value of `guid` from the previous step:
              <pre class="terminal">$ curl "http<span>s:</span>//OPS-MAN-FQDN/api/v0/deployed/products/PRODUCT-GUID/credentials/" \
                -X GET \
                -H "Authorization: Bearer UAA-ACCESS-TOKEN"</pre>
          1. Retrieve the MySQL admin password from the response to the above request.
  1. List the VMs in your deployment:
    <pre class="terminal">$ bosh -e DIRECTOR-IP --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    -d DEPLOYMENT-NAME \
    vms</pre>
  1. Run BOSH SSH:
    <pre class="terminal">$ bosh -e DIRECTOR-IP --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
    ssh</pre>
  1. Select the `mysql` VM to SSH into.
  1. From the `mysql` VM, run the following command:
    <pre class="terminal">$ sudo /var/vcap/packages/mariadb/bin/mysql -u root -p</pre> When prompted, enter the MySQL admin password.<br><br>
  1. At the MySQL prompt, run the following command:
    <pre class="terminal">mysql> use silk; drop table if exists subnets; drop table if exists gorp_migrations;</pre>
  1. Exit MySQL:
    <pre class="terminal">mysql> exit</pre>
  1. Exit the `mysql` VM:
    <pre class="terminal">$ exit</pre>
  1. List the VMs in your deployment:
    <pre class="terminal">$ bosh -e DIRECTOR-IP --ca-cert  PATH-TO-BOSH-SERVER-CERTIFICATE \
    -d DEPLOYMENT-NAME \
    vms</pre>
  1. SSH onto each `diego_database` VM and run the following command:
    <pre class="terminal">$ sudo su
    $ monit restart silk-controller</pre>

      Restored apps begin to start. The amount of time it takes for all apps to start depends on the number of app instances, the resources available to the underlying infrastructure, and the value of the **Max Inflight Container Starts** field in the Elastic Runtime tile.

1. If desired, scale the MySQL Server job back up to its previous number of instances by navigating to the **Resource Config** section of the Elastic Runtime tile. After scaling the job, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy.

##<a id='odb'></a> (Optional) Step 13: Restore On-Demand Service Instances

<p class="note"><strong>Note</strong>: These procedures restore the on-demand service instances but do not restore service instance data.</p>

If you have on-demand service instances provisioned by an on-demand service broker, perform the following steps to restore them after successfully restoring PCF:

1. Use the Cloud Foundry Command Line Interface (cf CLI) to target your PCF deployment:
  <pre class="terminal">
  $ cf api api.YOUR-SYSTEM-DOMAIN
  </pre>
1. Log in:
  <pre class="terminal">
  $ cf login
  </pre>
1. Perform the following steps to make a list of all the service instances provisioned by your on-demand service broker:
  1. List your service offerings:
    <pre class="terminal">
    $ cf curl /v2/services
    </pre>
  1. Record the GUID of the on-demand service offering you want to restore by examining the value for `guid` under `metadata`:
    <pre class="terminal">
     "metadata": {
        "guid": "ab2b01cc-2a22-525a-a333-e6e666a6aa66",
        "url": "/v2/services/ab2b01cc-2a22-525a-a333-e6e666a6aa66",
        "created_at": "2017-02-10T18:19:35Z",
        "updated_at": "2017-02-10T18:19:35Z"
    </pre>
  1. List all service plans for the service offering, replacing `SERVICE-OFFERING-GUID` with the GUID obtained in the previous step:
    <pre class="terminal">
    $ cf curl /v2/services/SERVICE-OFFERING-GUID/service_plans
    </pre>
  1. Record the GUID of each service plan by examining the value for `guid` under `metadata`.
  1. For each service plan, list all service instances:
    <pre class="terminal">
    $ cf curl /v2/service_plans/SERVICE-PLAN-GUID/service_instances
    </pre>
  1. Record the GUID of each service instance by examining the value for `guid` under `metadata`.
1. Perform the following steps to obtain the BOSH credentials used by your on-demand service broker:
  1. Navigate to `https://YOUR-OPS-MAN-FQDN/api/v0/staged/products` in a browser to obtain the product GUID of your tile.
  1. Navigate to `https://YOUR-OPS-MAN-FQDN/api/v0/staged/products/PRODUCT-GUID/manifest` to obtain your product's staged manifest.
  1. Copy the manifest into a file on your local machine called `manifest.json`.
  1. Run the following command to find the name of the deployment's on-demand broker instance group:
    <pre class="terminal">
    $ cat manifest.json | jq '(.instance_groups[].name )' | grep on-demand-broker | grep -v -E "register|smoke"
    > redis-on-demand-broker
    </pre>
  1. Run the following command to extract the BOSH credentials:
    <pre class="terminal">
    $ cat manifest.json | jq '(.instance_groups[] |
     select(.name == "redis-on-demand-broker").jobs[] |
     select(.name == "broker").properties.bosh.authentication.uaa )'
    </pre>
1. SSH into your Ops Manager VM. For more information, see the [SSH into Ops Manager](../trouble-advanced.html#ssh) section of the <em>Advanced Troubleshooting with the BOSH CLI</em> topic.
1. Using the BOSH credentials retrieved above, authenticate with your BOSH Director by running the following commands with the [BOSH CLI v2](https://bosh.io/docs/cli-v2.html):
  <pre class="terminal">
  $ export BOSH-CLIENT=YOUR-CLIENT-ID
  $ export BOSH_CLIENT_SECRET=YOUR-CLIENT-SECRET
  $ bosh alias-env director -e DIRECTOR-IP \
  --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE
  </pre>
1. Using the list of service instance GUIDs gathered above, deploy each instance with the following commands:
  <pre class="terminal">
  $ bosh -e director manifest \
  -d service-instance_SERVICE-INSTANCE-GUID > /tmp/manifest.yml
  $ bosh -e director \
  -d service-instance_SERVICE-INSTANCE-GUID deploy /tmp/manifest.yml
  </pre>
1. After deploying all service instances, remove the manifest from `tmp`.
  <pre class="terminal">
  $ rm /tmp/manifest.yml
  </pre>
1. Any Elastic Runtime apps bound to these services must be restarted to pick up the recreated service instances.

## <a id='non-tiles'></a> (Optional) Step 14: Restore Non-Tile Deployments

If you have any deployments that were deployed manually with the BOSH Director rather than through an Ops Manager Tile, perform the following steps to restore the VMs.

1. Identify the names of the deployments that you need to restore. Do not include the deployments from Ops Manager Tiles. Run the following command to obtain a list of all deployments on your BOSH Director:
  <pre class="terminal">
  $ bosh -e BOSH-DIRECTOR-IP \
      --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
      deployments
  </pre>
1. Run the following command for each deployment you need to restore:
  <pre class="terminal">
  $ bosh -n -e BOSH-DIRECTOR-IP \
      --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
      -d DEPLOYMENT-NAME \
      cck --resolution=recreate_vm
  </pre>
1. Run the following command to verify the status of the VMs in each deployment:
  <pre class="terminal">
  $ bosh -e BOSH-DIRECTOR-IP \
      --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE \
      -d DEPLOYMENT-NAME \
      vms
  </pre>
  The process state for all the VMs should now be `running`.

## <a id='removing-disks'></a> Remove Unused Disks

If `bosh cck` does not clean up all disk references, you must manually delete the disks from a previous deployment that will prevent recreated deployments from working.

<p class="note warning"><strong>Warning</strong>: This is a very destructive operation.</p>
<br><br>
To delete the disks, perform one of the following procedures:

* Use the BOSH CLI to delete the disks by performing the following steps:
  1. Target the redeployed BOSH Director using the BOSH CLI by performing the procedures in [Step 6: Retrieve BOSH Director Address and Credentials](#retrieve).
  1. List the deployments by running the following command:
    <pre class="terminal">
    $ bosh -e DIRECTOR-IP \
    --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE deployments
    </pre>
  1. Delete each deployment with the following command:
    <pre class="terminal">
    $ bosh -d DEPLOYMENT-NAME delete-deployment
    </pre>
* Log in to your IaaS account and delete the disks manually. Run the following command to retrieve a list of disk IDs:
  <pre class="terminal">
  $ bosh -e DIRECTOR-IP \
  --ca-cert PATH-TO-BOSH-SERVER-CERTIFICATE instances
  </pre>

Once the disks are deleted, continue with [Step 9: Remove Stale Cloud IDs for All Deployments](#bosh-cck).
